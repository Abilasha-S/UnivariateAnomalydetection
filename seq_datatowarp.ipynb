{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 26589,
     "status": "ok",
     "timestamp": 1595993780775,
     "user": {
      "displayName": "Abilasha S",
      "photoUrl": "",
      "userId": "17146042062219803973"
     },
     "user_tz": -330
    },
    "id": "agVKCjJroVz7",
    "outputId": "2c1ee518-3f1f-4263-f17f-8c9f424897ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: window-slider in /home/abilasha/anaconda3/lib/python3.7/site-packages (0.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/abilasha/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: dtaidistance in /home/abilasha/anaconda3/lib/python3.7/site-packages (1.2.3)\n",
      "Requirement already satisfied: numpy in /home/abilasha/anaconda3/lib/python3.7/site-packages (from dtaidistance) (1.19.0)\n",
      "Requirement already satisfied: cython in /home/abilasha/anaconda3/lib/python3.7/site-packages (from dtaidistance) (0.29.12)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/abilasha/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install window-slider\n",
    "!pip install dtaidistance\n",
    "from dtaidistance import dtw\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "executionInfo": {
     "elapsed": 27495,
     "status": "ok",
     "timestamp": 1595993782745,
     "user": {
      "displayName": "Abilasha S",
      "photoUrl": "",
      "userId": "17146042062219803973"
     },
     "user_tz": -330
    },
    "id": "s6AHqd6ujph4",
    "outputId": "ad912fcf-6a9e-4f6d-a564-c936d9f0da1f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/abilasha/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abilasha/anaconda3/lib/python3.7/site-packages/sklearn/utils/deprecation.py:143: FutureWarning: The sklearn.neighbors.lof module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.neighbors. Anything that cannot be imported from sklearn.neighbors is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import scipy.io\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import join, getsize\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score, precision_recall_curve, average_precision_score, cohen_kappa_score,confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from window_slider import Slider\n",
    "import sys\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "np.set_printoptions(suppress=True)\n",
    "from sklearn.neighbors import lof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQFEt9bxlF8U"
   },
   "outputs": [],
   "source": [
    "bucket_size = 30\n",
    "# bucket_size = 25\n",
    "length_data=dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hCv5DviWfwc"
   },
   "outputs": [],
   "source": [
    "def ReadDataset_withlab(_file_name, _normalize=True):\n",
    "\n",
    "    abnormal = pd.read_csv(_file_name)\n",
    "    abnormal=abnormal.values\n",
    "\n",
    "    abnormal_data = abnormal[:,0]\n",
    "    abnormal_label = abnormal[:,1]\n",
    "\n",
    "  \n",
    "    return abnormal_data,abnormal_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07nMc1ylhKL-"
   },
   "outputs": [],
   "source": [
    "def ReadDataset_seq(_file_name, _normalize=True):\n",
    "\n",
    "\n",
    "    df = pd.read_csv(_file_name,header=None)\n",
    "    abnormal = df.values\n",
    "\n",
    "    abnormal_data = abnormal[:,:-1] #change as position of class labels\n",
    "#     abnormal_data = abnormal[:,1:]\n",
    "    \n",
    "\n",
    "    abnormal_label = abnormal[:,-1]\n",
    "#     abnormal_label = abnormal[:,0]\n",
    "\n",
    "    print(\"anomaly\",(len(np.where(abnormal_label==-1)[0])))\n",
    "\n",
    "\n",
    "    # Normal = 1, Abnormal = -1\n",
    "    \n",
    "    return abnormal_data,len(abnormal_data),abnormal_label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadDataset_roll(_file_name_annot,anom_size):\n",
    "    bucket_size=50\n",
    "    base=os.path.basename(_file_name_annot) \n",
    "    base_name=base.split('Annotations_')[1]\n",
    "    file_name=os.path.join(os.path.dirname(_file_name_annot),base_name)\n",
    "    label_new=[]\n",
    "    value=[]\n",
    "    with open(file_name) as infile:\n",
    "        value = [float(line.strip('\\n')) for line in infile if line]\n",
    "    abnormal_data=np.array(value)\n",
    "    print(\"org len\",len(abnormal_data))\n",
    "    \n",
    "    with open(_file_name_annot) as infile:\n",
    "            indice = [int(line.strip('\\n')) for line in infile if line]\n",
    "        \n",
    "    abnormal_label=np.ones(len(abnormal_data))\n",
    "\n",
    "    for i in indice:\n",
    "                abnormal_label[i:i+anom_size]=-1\n",
    "       \n",
    "#     bucket_size=anom_size+20  \n",
    "    abnorm_data=np.empty((0,bucket_size),float)\n",
    "    abnorm_label=np.empty((0,bucket_size),float)\n",
    "    overlap_count = 0\n",
    "    slider = Slider(bucket_size,overlap_count)\n",
    "    slider1 = Slider(bucket_size,overlap_count)\n",
    "    slider.fit(abnormal_data) \n",
    "    slider1.fit(abnormal_label)\n",
    "    plt.plot(abnormal_data)\n",
    "    plt.show()\n",
    "    plt.plot(abnormal_label)\n",
    "    plt.show()\n",
    "    while True:\n",
    "        window_data1 = slider.slide() \n",
    "        window_label1= slider1.slide()\n",
    "        if  (len(window_data1)<bucket_size) and len(window_data1)>bucket_size/2:\n",
    "          window_data=np.pad(window_data1,[0,(bucket_size-len(window_data1))])\n",
    "          window_label=np.pad(window_label1,[0,(bucket_size-len(window_label1))])\n",
    "          abnorm_data= np.append(abnorm_data,[window_data], axis=0)\n",
    "          abnorm_label= np.append(abnorm_label,[window_label], axis=0)\n",
    "          break\n",
    "        else:\n",
    "          if len(window_data1)==0 or len(window_data1)<bucket_size/2:\n",
    "                break\n",
    "          abnorm_data=np.append(abnorm_data,[window_data1], axis=0)\n",
    "          abnorm_label=np.append(abnorm_label,[window_label1], axis=0)\n",
    "            \n",
    "    label=np.sign(np.sum(abnorm_label,axis=1))\n",
    "    combined_data=np.column_stack((label,abnorm_data))\n",
    "\n",
    "    return combined_data, anom_count\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "              anom_track=dict()\n",
    "              for root, dirs, files in os.walk('/dataset/RealDatasets_T2G/'):\n",
    "                    for dir in dirs:                        \n",
    "                        print(\"root=\",root)\n",
    "                        print(\"dir=\",dir)  \n",
    "                        anom_dict={'Marotta_Valve_Tek_14': 128,'Marotta_Valve_Tek_17': 128, 'chfdbchf15': 200, 'ann_gun_CentroidA_1': 150, 'Patient_respiration2': 150, 'Patient_respiration': 100, 'Marotta_Valve_Tek_16': 128, 'dutch_power_demand': 800}\n",
    "                        for file in glob.glob(str(root+dir+'/MBA_*.txt')): \n",
    "                                        print(file)\n",
    "                                        abnormal_data, a_count = ReadDataset_roll_mba(file)\n",
    "                                        base=os.path.basename(file)\n",
    "                                        base=base.split('.txt')[0]\n",
    "                                        anom_track[base]=a_count\n",
    "                                        print(np.shape(abnormal_data))\n",
    "                                        np.savetxt(str(file.split('.txt')[0]+'.csv'), abnormal_data, delimiter=',',fmt='%10.5f')\n",
    "                        for file in glob.glob(str(root+dir+'/Annotations_*.txt')): \n",
    "                                        base=os.path.basename(file)\n",
    "                                        base=base.split('Annotations_')[1]\n",
    "                                        base=base.split('.txt')[0]\n",
    "                                        print(file)\n",
    "                                        abnormal_data, a_count = ReadDataset_roll(file,anom_dict[base])\n",
    "                                        anom_track[base]=a_count\n",
    "                                        print(np.shape(abnormal_data))\n",
    "                                        np.savetxt(os.path.join(root,dir,str(base+'.csv')), abnormal_data, delimiter=',',fmt='%10.5f')\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 582
    },
    "executionInfo": {
     "elapsed": 1851,
     "status": "error",
     "timestamp": 1595993873449,
     "user": {
      "displayName": "Abilasha S",
      "photoUrl": "",
      "userId": "17146042062219803973"
     },
     "user_tz": -330
    },
    "id": "dULBUi6dhcW8",
    "outputId": "038a7fff-07d3-4414-daa8-b206ea624405"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root= /home/abilasha/Downloads/NAB/data_warp\n",
      "dir= artificial_seq\n",
      "['out_warpbest_model.h5', 'artificial5_seq.csv', 'artificial_seq.csv', 'artificial_seq_normal.csv', 'artificial3_seq.csv']\n",
      "/home/abilasha/Downloads/NAB/data_warp/artificial_seq/artificial5_seq.csv\n",
      "anomaly 14\n",
      "(100, 18)\n",
      "/home/abilasha/Downloads/NAB/data_warp/artificial_seq/artificial_seq.csv\n",
      "anomaly 14\n",
      "(100, 90)\n",
      "/home/abilasha/Downloads/NAB/data_warp/artificial_seq/artificial_seq_normal.csv\n",
      "anomaly 14\n",
      "(94, 90)\n",
      "/home/abilasha/Downloads/NAB/data_warp/artificial_seq/artificial3_seq.csv\n",
      "anomaly 14\n",
      "(100, 30)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "              for root, dirs, files in os.walk('/dataset/data_warp'):\n",
    "#                     dirs=['artificial_seq']  ######change label position new and old\n",
    "                    dirs=['seq_data']\n",
    "#                     dirs=['Wafer','HandOutlines']\n",
    "#                     dirs=['DistalPhalanxOutlineCorrect','MiddlePhalanxOutlineCorrect','ProximalPhalanxOutlineCorrect','Earthquakes','PhalangesOutlinesCorrect','Strawberry','ToeSegmentation1','ToeSegmentation2','DodgerLoopGame','ECG5000','SyntheticControl','ECG200','Wafer','HandOutlines']\n",
    "#                     dirs=['TwoLeadECG','Computers','Worms','Yoga','Ford','Lightning2','InsectWingbeatSound']\n",
    "                    for dir in dirs:\n",
    "                        \n",
    "                        s_precision = []\n",
    "                        s_recall = []\n",
    "                        s_f1 = []\n",
    "                        s_roc_auc = []\n",
    "                        s_pr_auc = []\n",
    "                        s_cks = []\n",
    "                        \n",
    "                        print(\"root=\",root)\n",
    "                        print(\"dir=\",dir)\n",
    "                        \n",
    "                        for root1,dir1,files in os.walk(os.path.join(root,dir)):\n",
    "                             print(files)\n",
    "                             \n",
    "                             for file in files: \n",
    "                                if file.endswith('.csv'):\n",
    "                                    file_name = os.path.join(root,dir,file)\n",
    "                                    print(file_name)\n",
    "                                \n",
    "                                # abnormal_data,length ,abnormal_label = ReadDataset_withlab(file_name)\n",
    "                                    abnormal_data,length,abnormal_label = ReadDataset_seq(file_name)\n",
    "                                    length_data[file.split('.csv')[-2]]=length\n",
    "                                    print(np.shape(abnormal_data))\n",
    "                                # print(np.shape(abnormal_label))\n",
    "                                    file_name1=os.path.join(os.path.join(root,dir),\"out_warp\",str(os.path.splitext(file)[0])+\"_warp.csv\")\n",
    "                                    np.savetxt(file_name1, abnormal_data, delimiter=',',fmt='%10.5f')\n",
    "#                                     file_name2=os.path.join(os.path.join(root,dir),\"out_warp\",str(os.path.splitext(file)[0])+\"_warp_label.csv\")\n",
    "#                                 # # file_name2=os.path.join(os.path.join(root,dir),\"out_warp\",str(os.path.splitext(file)[0])+\"_warp_revlabel.csv\")\n",
    "#                                     np.savetxt(file_name2, abnormal_label, delimiter=',',fmt='%d')\n",
    "                             break    \n",
    "                    break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 42084,
     "status": "ok",
     "timestamp": 1595927428145,
     "user": {
      "displayName": "Abilasha S",
      "photoUrl": "",
      "userId": "17146042062219803973"
     },
     "user_tz": -330
    },
    "id": "H_sIbBefyRdq",
    "outputId": "305ec20f-f007-40c7-dd59-f6fe88bd15df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'artificial5_seq': 100, 'artificial_seq': 100, 'artificial_seq_normal': 94, 'artificial3_seq': 100}\n"
     ]
    }
   ],
   "source": [
    "print(length_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ps-_4ROUPiuL"
   },
   "outputs": [],
   "source": [
    "def ReadDataset_rep(_file_name,rep,root,dir, _normalize=True):\n",
    "\n",
    "\n",
    "    df = pd.read_csv(_file_name,header=None)\n",
    "    abnormal_data = df.values\n",
    "\n",
    "    \n",
    "    return abnormal_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNMuAjkjPVg_"
   },
   "source": [
    "# **NN Method warp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A9zOtvnrPZuY",
    "outputId": "cca3be26-1ec0-4adb-c8c6-32cc0c079108"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root= /home/abilasha/Downloads/NAB/data_warp/\n",
      "dir= HandOutlines\n",
      "['0.csv', 'HandOutlines_warp_rep.csv']\n",
      "filename /home/abilasha/Downloads/NAB/data_warp/HandOutlines/copy/HandOutlines_warp_rep.csv\n",
      "--- 5.29523491859436 seconds ---\n",
      "root= /home/abilasha/Downloads/NAB/data_warp/\n",
      "dir= Wafer\n",
      "['Wafer_warp_rep.csv', '0.csv']\n",
      "filename /home/abilasha/Downloads/NAB/data_warp/Wafer/copy/Wafer_warp_rep.csv\n",
      "--- 300.52293062210083 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def RunModel(_file_name,root,dir,lenth):\n",
    "    rep=1\n",
    "    abnormal_data = ReadDataset_rep(_file_name,rep,root,dir) \n",
    "    y_pred_arr1= np.empty((0,lenth), int)\n",
    "    y_pred_1=-1*np.ones(lenth,int)\n",
    "    score_1=np.zeros(lenth)\n",
    "    y_pred_arr2= np.empty((0,lenth), int)\n",
    "\n",
    "    score_2=np.zeros(lenth)\n",
    "    y_pred_2=-1*np.ones(lenth,int)\n",
    "    y_pred_arr3= np.empty((0,lenth), int)\n",
    "\n",
    "    y_pred_3=-1*np.ones(lenth,int)\n",
    "    score_3=np.zeros(lenth)\n",
    "    y_pred_arr4= np.empty((0,lenth), int)\n",
    "\n",
    "    score_4=np.zeros(lenth)\n",
    "    y_pred_4=-1*np.ones(lenth,int)\n",
    "    y_pred_arr5= np.empty((0,lenth), int)\n",
    "\n",
    "    y_pred_5=-1*np.ones(lenth,int)\n",
    "    score_5=np.zeros(lenth)\n",
    "    \n",
    "    \n",
    "    d_nn1=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn2=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn3=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn4=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn5=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    # d_nn3_arg=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    # d_nn3_max=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    # d_nn3_arg_max=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    w1=[]\n",
    "    w2=[]\n",
    "    w3=[]\n",
    "    w4=[]\n",
    "    w5=[]\n",
    "    k=0\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "#     clf = lof.LocalOutlierFactor(n_neighbors=1, n_jobs=2,contamination=0.1)\n",
    "#     y_temp = clf._fit_predict(abnormal_data)\n",
    "#     score=clf.negative_outlier_factor_\n",
    "#     dist=-1*score\n",
    "#     d_nn1=dist \n",
    "#     clf = lof.LocalOutlierFactor(n_neighbors=3, n_jobs=2,contamination=0.1)\n",
    "#     y_temp = clf._fit_predict(abnormal_data)\n",
    "#     score=clf.negative_outlier_factor_\n",
    "#     dist=-1*score \n",
    "#     d_nn2=dist\n",
    "#     clf = lof.LocalOutlierFactor(n_neighbors=5, n_jobs=2,contamination=0.1)\n",
    "#     y_temp = clf._fit_predict(abnormal_data)\n",
    "#     score=clf.negative_outlier_factor_\n",
    "#     dist=-1*score\n",
    "#     d_nn3=dist\n",
    "#     clf = lof.LocalOutlierFactor(n_neighbors=7, n_jobs=2,contamination=0.1)\n",
    "#     y_temp = clf._fit_predict(abnormal_data)\n",
    "#     score=clf.negative_outlier_factor_\n",
    "#     dist=-1*score \n",
    "#     d_nn4=dist\n",
    "#     clf = lof.LocalOutlierFactor(n_neighbors=11, n_jobs=2,contamination=0.1)\n",
    "#     y_temp = clf._fit_predict(abnormal_data)\n",
    "#     score=clf.negative_outlier_factor_\n",
    "#     dist=-1*score\n",
    "#     d_nn5=dist\n",
    "\n",
    "    for r,t in zip(abnormal_data,range(len(abnormal_data))):                    ####knn\n",
    "            dist=[]\n",
    "\n",
    "            # dist_arg=[]\n",
    "            for o in abnormal_data:\n",
    "                    dist.append(np.linalg.norm(r-o))              \n",
    "                    \n",
    "            dist_arg=(np.argsort(dist))\n",
    "            dist=np.sort(dist)\n",
    "#             print(dist_arg)\n",
    "            d_nn1[k]=dist[1]\n",
    "            d_nn2[k]=dist[3]\n",
    "            d_nn3[k]=dist[5]\n",
    "            d_nn4[k]=dist[7]\n",
    "            d_nn5[k]=dist[11]\n",
    "#......................score to mid....................................    for point        \n",
    "#             y_predtemp_1[k][(mid-2):(mid+2)]=dist[1]\n",
    "#             y_predtemp_2[k][(mid-2):(mid+2)]=dist[3]\n",
    "#             y_predtemp_3[k][(mid-2):(mid+2)]=dist[5]\n",
    "#.....................................................................\n",
    "            # d_nn3_arg[k]=dist_arg[1]\n",
    "            # d_nn3_max[k]=dist[-1]\n",
    "            # d_nn3_arg_max[k]=dist_arg[-1]\n",
    "            # # d_nn3[k]=dist[9]\n",
    "\n",
    "            k+=1                                             ##knn\n",
    "            \n",
    "            \n",
    "#     print(\"--- %s seconds ---\" % (time.time() - start_time))   \n",
    "#     score_1=apprecons1(y_predtemp_1,lenth,bucket_size)    # for point\n",
    "#     score_2=apprecons1(y_predtemp_2,lenth,bucket_size)\n",
    "#     score_3=apprecons1(y_predtemp_3,lenth,bucket_size)\n",
    "\n",
    "#     w1=np.argsort(score_1)[::-1]     # for point\n",
    "#     w2=np.argsort(score_2)[::-1]\n",
    "#     w3=np.argsort(score_3)[::-1]\n",
    "\n",
    "    \n",
    "\n",
    "#     w1=np.argsort(d_nn1)[::-1]                  # for seq\n",
    "#     w2=np.argsort(d_nn2)[::-1]\n",
    "#     w3=np.argsort(d_nn3)[::-1]\n",
    "#     w4=np.argsort(d_nn4)[::-1]\n",
    "#     w5=np.argsort(d_nn5)[::-1]\n",
    "    \n",
    "    return  d_nn1,d_nn2,d_nn3,d_nn4,d_nn5\n",
    "# #     print(w2)\n",
    "    \n",
    "#     # # w_min=w3[-1]\n",
    "#     # # w_max=np.argsort(d_nn3_max)[::-1][0]\n",
    "#     # # w_min_n=d_nn3_arg[w_min]\n",
    "#     # # w_max_n=d_nn3_arg_max[w_max]\n",
    "#     # w3_arg=(np.argsort(d_nn3))[::-1]\n",
    "    \n",
    "# #     for i in range(1,math.ceil(1*lenth),1):\n",
    "#     for i in range(1,math.ceil(1*len(abnormal_data)),1):\n",
    "      \n",
    "#       for e,j,k,l,m in zip(w1[0:i],w2[0:i],w3[0:i],w4[0:i],w5[0:i]): \n",
    "#         if e<len(abnormal_data):\n",
    "# #         if e<lenth:\n",
    "          \n",
    "#           y_pred_1[e]=1\n",
    "#           y_pred_2[j]=1\n",
    "#           y_pred_3[k]=1\n",
    "#           y_pred_4[l]=1\n",
    "#           y_pred_5[m]=1\n",
    "#           # # y_predtemp_3[e][(mid-2):(mid+2)]=1\n",
    "#           # # y_predtemp_3[e][mid]=1\n",
    "\n",
    "#       y_pred_arr1=np.append(y_pred_arr1,np.expand_dims(y_pred_1,0),axis=0)\n",
    "#       y_pred_arr2=np.append(y_pred_arr2,np.expand_dims(y_pred_2,0),axis=0)\n",
    "#       # # y_pred_3=apprecons(y_predtemp_3,lenth,bucket_size)\n",
    "#       y_pred_arr3=np.append(y_pred_arr3,np.expand_dims(y_pred_3,0),axis=0)\n",
    "#       y_pred_arr4=np.append(y_pred_arr4,np.expand_dims(y_pred_4,0),axis=0)\n",
    "#       y_pred_arr5=np.append(y_pred_arr5,np.expand_dims(y_pred_5,0),axis=0)\n",
    "      \n",
    "#       # print(\"anom y-pred\",len(np.where(y_pred==1)[0]))\n",
    "   \n",
    "   \n",
    "#     # return  y_pred_arr3,d_nn3,w_min,w_max,w_min_n,w_max_n,w3\n",
    "#     return  y_pred_arr1,d_nn1,y_pred_arr2,d_nn2,y_pred_arr3,d_nn3,y_pred_arr4,d_nn4,y_pred_arr5,d_nn5\n",
    "# #     return  y_pred_arr1,score_1,y_pred_arr2,score_2,y_pred_arr3,score_3\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "              \n",
    "              for root, dirs, files in os.walk('/dataset/data_warp/'):\n",
    "                     dirs=['seq_data']\n",
    "#                      dirs=['artificial_seq']\n",
    "#                      dirs=['DistalPhalanxOutlineCorrect','MiddlePhalanxOutlineCorrect','ProximalPhalanxOutlineCorrect','Earthquakes','PhalangesOutlinesCorrect','Strawberry','ToeSegmentation1','ToeSegmentation2','DodgerLoopGame','ECG5000','SyntheticControl','ECG200','HandOutlines','Wafer']\n",
    "#                     dirs=['TwoLeadECG','Computers','Worms','Yoga','Ford','Lightning2','InsectWingbeatSound']\n",
    "                     for dir in dirs:\n",
    "                        \n",
    "                        \n",
    "                        print(\"root=\",root)\n",
    "                        print(\"dir=\",dir)\n",
    "                        warp=\"copy\"\n",
    "                        dir_=os.path.join(root,dir,warp)\n",
    "                        \n",
    "                        for root1,dir1,files in os.walk(dir_):\n",
    "                            print(files)\n",
    "#                             files=['dutch_power_demand_warp_rep.csv', 'ann_gun_CentroidA_1_warp_rep.csv', 'Patient_respiration2_warp_rep.csv', 'chfdbchf15_warp_rep.csv', 'Marotta_Valve_Tek_16_warp_rep.csv', 'Patient_respiration_warp_rep.csv', 'Marotta_Valve_Tek_17_warp_rep.csv','Marotta_Valve_Tek_14_warp_rep.csv']\n",
    "#                             files=['MBA_ECG_820_warp_rep.csv', 'MBA_ECG_805_warp_rep.csv','MBA_ECG_803_warp_rep.csv', 'MBA_ECG_806_warp_rep.csv']\n",
    "                            for file in files:\n",
    "                              \n",
    "                              \n",
    "                              if file.endswith(\"_warp_rep.csv\"): \n",
    "                                file_name = os.path.join(dir_,file)\n",
    "                                print(\"filename\",file_name)\n",
    "                               \n",
    "                                dist_1,dist_2,dist_3,dist_4,dist_5 = RunModel(file_name,root,dir,length_data[file.split('_warp_rep.csv')[-2]])\n",
    "                                abnormal_data,length,abnormal_label = ReadDataset_seq(os.path.join(root,dir,str(file.split('_warp_rep.csv')[0]+'.csv')))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_1)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_1)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                auc_1=metrics.auc(recall,precision)\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_2)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_2)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                auc_3=metrics.auc(recall,precision)\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_3)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_3)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                auc_5=metrics.auc(recall,precision)\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_4)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_4)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                auc_7=metrics.auc(recall,precision)\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_5)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_5)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                auc_11=metrics.auc(recall,precision)\n",
    "                                mean_auc=np.mean([auc_3,auc_5,auc_7,auc_11])\n",
    "                                print(mean_auc)\n",
    "\n",
    "                                \n",
    "                            break\n",
    "                               \n",
    "                     break         \n",
    "                                \n",
    "                                \n",
    "                                 \n",
    "                                \n",
    " \n",
    "                                \n",
    "                                 \n",
    "                                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RunModel(w,file,_file_name,root,dir,lenth):\n",
    "    rep=1\n",
    "\n",
    "    abnormal_data = ReadDataset_rep(_file_name,rep,root,dir) \n",
    "    y_pred_arr1= np.empty((0,lenth), int)\n",
    "\n",
    "    y_pred_1=-1*np.ones(lenth,int)\n",
    "    score_1=np.zeros(lenth)\n",
    "    y_pred_arr2= np.empty((0,lenth), int)\n",
    "\n",
    "    score_2=np.zeros(lenth)\n",
    "    y_pred_2=-1*np.ones(lenth,int)\n",
    "    y_pred_arr3= np.empty((0,lenth), int)\n",
    "\n",
    "    y_pred_3=-1*np.ones(lenth,int)\n",
    "    score_3=np.zeros(lenth)\n",
    "    y_pred_arr4= np.empty((0,lenth), int)\n",
    "\n",
    "    score_4=np.zeros(lenth)\n",
    "    y_pred_4=-1*np.ones(lenth,int)\n",
    "    y_pred_arr5= np.empty((0,lenth), int)\n",
    "\n",
    "    y_pred_5=-1*np.ones(lenth,int)\n",
    "    score_5=np.zeros(lenth)\n",
    "    \n",
    "    \n",
    "    d_nn1=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn2=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn3=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn4=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn5=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    # d_nn3_arg=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    # d_nn3_max=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    # d_nn3_arg_max=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    w1=[]\n",
    "    w2=[]\n",
    "    w3=[]\n",
    "    w4=[]\n",
    "    w5=[]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    k=0\n",
    "\n",
    "#     dist=np.zeros((len(abnormal_data),len(abnormal_data)))      ###for lof\n",
    "    for r,t in zip(abnormal_data,range(len(abnormal_data))):\n",
    "            dist=[]  ###for knn\n",
    "            temp=[]\n",
    "            dist_arg=[]\n",
    "            for o in abnormal_data:\n",
    "#                 dist.append(dtw.distance(r,o,window=w))  \n",
    "#                   dist.append(dtw.distance_fast(r,o)) \n",
    "                    dist.append(dtw.distance_fast(r,o,window=w))      ## for knn               \n",
    "#                     temp.append(dtw.distance_fast(r,o,window=1))    ##lof\n",
    "\n",
    "            # # for lof dtw\n",
    "#             dist[t]=temp \n",
    "        \n",
    "        # #for knn seq \n",
    "                            \n",
    "            ## dist_arg=(np.argsort(dist))\n",
    "            dist=np.sort(dist)\n",
    "            \n",
    "            d_nn1[k]=dist[1]                     \n",
    "            d_nn2[k]=dist[3]\n",
    "            d_nn3[k]=dist[5]\n",
    "            d_nn4[k]=dist[7]\n",
    "            d_nn5[k]=dist[11]\n",
    "            \n",
    "            # y_predtemp_1[k][(mid-2):(mid+2)]=dist[1] #for point knn\n",
    "            # y_predtemp_2[k][(mid-2):(mid+2)]=dist[3]\n",
    "            # y_predtemp_3[k][(mid-2):(mid+2)]=dist[5]\n",
    "            \n",
    "\n",
    "            # d_nn3_arg[k]=dist_arg[1]\n",
    "            # d_nn3_max[k]=dist[-1]\n",
    "            # d_nn3_arg_max[k]=dist_arg[-1]\n",
    "            # # d_nn3[k]=dist[9]\n",
    "            \n",
    "            k+=1                                                ##knn\n",
    "            \n",
    "\n",
    "    return  y_pred_arr1,d_nn1,y_pred_arr2,d_nn2,y_pred_arr3,d_nn3,y_pred_arr4,d_nn4,y_pred_arr5,d_nn5\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "              \n",
    "              for root, dirs, files in os.walk('/dataset/data_warp/'):\n",
    "                    i=0\n",
    "                    dirs=['DistalPhalanxOutlineCorrect','MiddlePhalanxOutlineCorrect','ProximalPhalanxOutlineCorrect','Earthquakes','PhalangesOutlinesCorrect','Strawberry','ToeSegmentation1','ToeSegmentation2','DodgerLoopGame','ECG5000','SyntheticControl','ECG200']\n",
    "#                     w=[2,1,1,22,0,0,8,5,2,1,6,2,1,1]\n",
    "#                     dirs=['ToeSegmentation1','DodgerLoopGame','Earthquakes'] \n",
    "#                     w=[1,2,11]\n",
    "#                     w=[1,5,1,29,1,2,12,1,21,1,2,1]\n",
    "#                     dirs=['HandOutlines','Wafer'] \n",
    "#                     dirs=['seq_data']\n",
    "#                     dirs=['TwoLeadECG','Computers','Worms','Yoga','Ford','Lightning2','InsectWingbeatSound']\n",
    "                    for dir in dirs: \n",
    "                        print(\"root=\",root)\n",
    "                        print(\"dir=\",dir)\n",
    "                        dir_=os.path.join(root,dir,\"out_warp\")\n",
    "                      \n",
    "                        for root1,dir1,files in os.walk(dir_):\n",
    "                            print(files)\n",
    "#                             w=[7,13,20,20,16,8,8,9]\n",
    "#                             files=['dutch_power_demand_warp.csv','Marotta_Valve_Tek_14_warp.csv','ann_gun_CentroidA_1_warp.csv',  'chfdbchf15_warp.csv','Patient_respiration2_warp.csv', 'Patient_respiration_warp.csv', 'Marotta_Valve_Tek_16_warp.csv',  'Marotta_Valve_Tek_17_warp.csv']\n",
    "#                             files=['ann_gun_CentroidA_1_warp.csv']\n",
    "#                             w=[6,6,4,10]\n",
    "#                             files=['MBA_ECG_820_warp.csv', 'MBA_ECG_806_warp.csv', 'MBA_ECG_805_warp.csv', 'MBA_ECG_803_warp.csv']\n",
    "                            for file in files:\n",
    "                              i=0\n",
    "                              best_mean=0\n",
    "                              if file.endswith(\"_warp.csv\"): \n",
    "                                file_name = os.path.join(dir_,file)\n",
    "                                print(\"filename\",file_name)\n",
    "                                abnormal_data,length,abnormal_label = ReadDataset_seq(os.path.join(root,dir,str(file.split('_warp.csv')[0]+'.csv')))\n",
    "#                                 for w in range(1,int(np.floor(0.1*len(abnormal_data[0])))):\n",
    "#                                     y_pred3, dist_3,w_min,w_max,w_min_n,w_max_n,w3 = RunModel(file_name,root,dir,length_data[file.split('_warp.csv')[-2]])\n",
    "                                y_pred1, dist_1,y_pred2, dist_2,y_pred3, dist_3,y_pred4, dist_4,y_pred5, dist_5 = RunModel(0.1*len(abnormal_data[0]),file,file_name,root,dir,length_data[file.split('_warp.csv')[-2]])                         \n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_1)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_1)\n",
    "#                                     auc_1=metrics.auc(fpr, tpr)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_2)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_2)\n",
    "#                                     auc_2=metrics.auc(fpr, tpr)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_3)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_3)\n",
    "#                                     auc_3=metrics.auc(fpr, tpr)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_4)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_4)\n",
    "#                                     auc_4=metrics.auc(fpr, tpr)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_5)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_5)\n",
    "#                                     auc_5=metrics.auc(fpr, tpr)\n",
    "#                                     mean_auc=np.mean([auc_1,auc_2,auc_3,auc_4,auc_5])\n",
    "#                                     if best_mean<auc_1:\n",
    "#                                             best_mean=auc_1\n",
    "#                                             best_w=w\n",
    "#                                 print(best_w)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "\n",
    "#                             i=i+1\n",
    "                            break\n",
    "                    i=i+1          \n",
    "                    break         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_autoencoder(time_series, name, seed):\n",
    "        #encoder\n",
    "        # conv16@7\n",
    "        x = Conv1D(filters=16, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=initializers.he_normal(seed=seed), data_format='channels_last')(time_series)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        # conv16@5\n",
    "        x = Conv1D(filters=16, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=initializers.he_normal(seed=seed), data_format=\"channels_last\")(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        maxpool1_fanin = x._keras_shape[-2]\n",
    "        # maxpooling@3\n",
    "        x = MaxPool1D(pool_size=3, padding='same')(x)\n",
    "        # conv32@3\n",
    "        x = Conv1D(filters=32, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=initializers.he_normal(seed=seed), data_format=\"channels_last\")(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        # conv32@3\n",
    "        x = Conv1D(filters=32, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=initializers.he_normal(seed=seed), data_format=\"channels_last\")(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        # maxpooling@0.1L\n",
    "        maxpool2_fanin = x._keras_shape[-2]\n",
    "        pool_width = int(np.ceil(0.1 * maxpool2_fanin))\n",
    "        #signet pooling\n",
    "        x, mask = MaxPoolingWithArgmax1D(pool_size=pool_width, padding=\"same\")(x) #M,16\n",
    "        num_channels = x._keras_shape[-1]\n",
    "\n",
    "        x = Flatten()(x) # 16M length\n",
    "        num_nodes = x._keras_shape[-1]\n",
    "        # dense@rep_len\n",
    "        #rep = Dense(units=10, kernel_initializer=initializers.he_normal(seed=seed))(x)\n",
    "        rep = Dense(units=40, kernel_initializer=initializers.he_normal(seed=seed))(x)\n",
    "\n",
    "        # decoder\n",
    "        dense_upsample = Dense(units=num_nodes, kernel_initializer=initializers.he_normal(seed=seed))(rep) #160\n",
    "        x = Reshape((int(num_nodes/num_channels),num_channels))(dense_upsample) #M,16\n",
    "        # upsampling@0.1L\n",
    "        #x = UpSampling1D(size=pool_width)(x)\n",
    "        x = MaxUnpooling1D(pool_width)([x, mask])\n",
    "        upsample1_fanout = x._keras_shape[-2]\n",
    "        # crop excess from right end.\n",
    "        x = Cropping1D(cropping=(0, upsample1_fanout - maxpool2_fanin))(x) # L,16\n",
    "        # conv32@3\n",
    "        x = Conv1D(filters=32, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=initializers.he_normal(seed=seed), data_format=\"channels_last\")(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        # conv32@3\n",
    "        x = Conv1D(filters=16, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=initializers.he_normal(seed=seed), data_format=\"channels_last\")(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        # upsampling@3\n",
    "        x = UpSampling1D(size=3)(x)\n",
    "        # Crop excess from the right end.\n",
    "        upsample2_fanout = x._keras_shape[-2]\n",
    "        x = Cropping1D(cropping=(0, upsample2_fanout - maxpool1_fanin))(x)\n",
    "        # conv16@5\n",
    "        x = Conv1D(filters=16, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=initializers.he_normal(seed=seed), data_format=\"channels_last\")(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        # conv16@7\n",
    "        x = Conv1D(filters=1, kernel_size=7, strides=1, padding=\"same\", kernel_initializer=initializers.he_normal(seed=seed), data_format=\"channels_last\")(x)\n",
    "        out = Lambda(lambda y:y, name=name)(x)\n",
    "\n",
    "        return rep, out\n",
    "\n",
    "\n",
    "\n",
    "def RunModel(_abnormal_data, _file_name):\n",
    "        model=[]\n",
    "        model_path=os.path.dirname(_file_name)\n",
    "        series_length=len(_abnormal_data[0])\n",
    "        time_series_1 = Input(shape=(series_length,1))\n",
    "        _abnormal_org=_abnormal_data\n",
    "        _abnormal_org=np.expand_dims(_abnormal_org,axis=-1)\n",
    "        random.shuffle(_abnormal_data)\n",
    "        _abnormal_data=np.expand_dims(_abnormal_data,axis=-1)\n",
    "        rep_layer_1, AE1_out = _create_autoencoder(time_series_1, 'AE1', 1)\n",
    "\n",
    "        model = Model(inputs=time_series_1, outputs=[AE1_out])\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=100)\n",
    "        mc = ModelCheckpoint(str(model_path+'best_model.h5'), monitor='val_loss', mode='min', verbose=0, save_best_only=True)\n",
    "        losses = {\n",
    "                'AE1':'mean_squared_error',\n",
    "                }\n",
    "        model.compile(optimizer=Adam(lr=0.001), loss=losses)\n",
    "        model.fit(_abnormal_data,_abnormal_data, validation_split=0.2, epochs=2000,verbose=0,callbacks=[es,mc])\n",
    "\n",
    "        model1=load_model(str(model_path+'best_model.h5'),custom_objects={\"MaxPoolingWithArgmax1D\": layers.MaxPoolingWithArgmax1D,\"MaxUnpooling1D\": layers.MaxUnpooling1D})\n",
    "        inputs = model1.get_layer(index=0).input\n",
    "        output1 = model1.get_layer(index=12).output\n",
    "        output2=model1.get_layer(index=26).output\n",
    "        \n",
    "        outputs=[output1, output2]\n",
    "        intermediate_model = Model(inputs=inputs, outputs=outputs)\n",
    "        print(intermediate_model.summary())\n",
    "        rep, recons=intermediate_model.predict(_abnormal_org)\n",
    "        recons=np.squeeze(recons)\n",
    "        del model\n",
    "        del intermediate_model\n",
    "        return rep, recons\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    for root, dirs, _ in os.walk('/dataset/data_warp/'):\n",
    "#                 dirs=['DistalPhalanxOutlineCorrect','MiddlePhalanxOutlineCorrect','ProximalPhalanxOutlineCorrect','Earthquakes','PhalangesOutlinesCorrect','Strawberry','ToeSegmentation1','ToeSegmentation2','DodgerLoopGame','ECG5000','SyntheticControl','Wafer','HandOutlines']\n",
    "#                 dirs=['DistalPhalanxOutlineCorrect','MiddlePhalanxOutlineCorrect','ProximalPhalanxOutlineCorrect','Earthquakes','PhalangesOutlinesCorrect','Strawberry','ToeSegmentation1','ToeSegmentation2','DodgerLoopGame','ECG5000','SyntheticControl','ECG200']\n",
    "#                    \n",
    "#                 dirs=['artificial_seq']\n",
    "                dirs=['seq_data']\n",
    "                for dir in dirs:\n",
    "                    print(\"root=\",root)\n",
    "                    print(\"dir=\",dir)\n",
    "                    dir_=os.path.join(root,dir,\"out_warp\")\n",
    "                    for root1,dir1,files in os.walk(dir_):\n",
    "                    # for root1,dir1,files in os.walk(os.path.join(root,dir)):\n",
    "#                             print(files)\n",
    "                            files=['dutch_power_demand_warp.csv', 'ann_gun_CentroidA_1_warp.csv',  'chfdbchf15_warp.csv','Patient_respiration2_warp.csv', 'Patient_respiration_warp.csv', 'Marotta_Valve_Tek_16_warp.csv',  'Marotta_Valve_Tek_17_warp.csv','Marotta_Valve_Tek_14_warp.csv']\n",
    "                            for file in files:\n",
    "\n",
    "                              if file.endswith(\"_warp.csv\"):\n",
    "                              # if file.endswith(\".csv\"):\n",
    "                                file_name = os.path.join(dir_,file)\n",
    "                                # file_name = os.path.join(os.path.join(root,dir),file)\n",
    "#                                 print(\"filename\",file_name)\n",
    "\n",
    "                                final_error = []\n",
    "                                final_recons=[]\n",
    "                                abnormal_data = ReadDataset_seq(file_name)\n",
    "\n",
    "\n",
    "                                embed, recons = RunModel(np.array(abnormal_data),_file_name=file_name)\n",
    "\n",
    "                                file_name1=os.path.join(os.path.join(root,dir),\"ann/\",str(os.path.splitext(file)[0])+\"_recons.csv\")\n",
    "                                print(file_name1)\n",
    "                                np.savetxt(file_name1,recons, delimiter=',',fmt='%10.5f')\n",
    "\n",
    "                                file_name1=os.path.join(os.path.join(root,dir),\"ann/\",str(os.path.splitext(file)[0])+\"_embed.csv\")\n",
    "                                print(file_name1)\n",
    "                                np.savetxt(file_name1, embed, delimiter=',',fmt='%10.5f')\n",
    "\n",
    "\n",
    "                            break\n",
    "                break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def RunModel(_file_name,root,dir,lenth):\n",
    "    rep=1\n",
    "    abnormal_data = ReadDataset_rep(_file_name,rep,root,dir) \n",
    "    y_pred_arr1= np.empty((0,lenth), int)\n",
    "\n",
    "    y_pred_1=-1*np.ones(lenth,int)\n",
    "    score_1=np.zeros(lenth)\n",
    "    y_pred_arr2= np.empty((0,lenth), int)\n",
    "\n",
    "    score_2=np.zeros(lenth)\n",
    "    y_pred_2=-1*np.ones(lenth,int)\n",
    "    y_pred_arr3= np.empty((0,lenth), int)\n",
    "\n",
    "    y_pred_3=-1*np.ones(lenth,int)\n",
    "    score_3=np.zeros(lenth)\n",
    "    y_pred_arr4= np.empty((0,lenth), int)\n",
    "\n",
    "    score_4=np.zeros(lenth)\n",
    "    y_pred_4=-1*np.ones(lenth,int)\n",
    "    y_pred_arr5= np.empty((0,lenth), int)\n",
    "\n",
    "    y_pred_5=-1*np.ones(lenth,int)\n",
    "    score_5=np.zeros(lenth)\n",
    "    \n",
    "    \n",
    "    d_nn1=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn2=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn3=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn4=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn5=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    # d_nn3_arg=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    # d_nn3_max=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    # d_nn3_arg_max=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    w1=[]\n",
    "    w2=[]\n",
    "    w3=[]\n",
    "    w4=[]\n",
    "    w5=[]\n",
    "    k=0\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "#     clf = lof.LocalOutlierFactor(n_neighbors=1, n_jobs=2,contamination=0.1)\n",
    "#     y_temp = clf._fit_predict(abnormal_data)\n",
    "#     score=clf.negative_outlier_factor_\n",
    "#     dist=-1*score\n",
    "#     d_nn1=dist \n",
    "#     clf = lof.LocalOutlierFactor(n_neighbors=3, n_jobs=2,contamination=0.1)\n",
    "#     y_temp = clf._fit_predict(abnormal_data)\n",
    "#     score=clf.negative_outlier_factor_\n",
    "#     dist=-1*score \n",
    "#     d_nn2=dist\n",
    "#     clf = lof.LocalOutlierFactor(n_neighbors=5, n_jobs=2,contamination=0.1)\n",
    "#     y_temp = clf._fit_predict(abnormal_data)\n",
    "#     score=clf.negative_outlier_factor_\n",
    "#     dist=-1*score\n",
    "#     d_nn3=dist\n",
    "#     clf = lof.LocalOutlierFactor(n_neighbors=7, n_jobs=2,contamination=0.1)\n",
    "#     y_temp = clf._fit_predict(abnormal_data)\n",
    "#     score=clf.negative_outlier_factor_\n",
    "#     dist=-1*score \n",
    "#     d_nn4=dist\n",
    "#     clf = lof.LocalOutlierFactor(n_neighbors=11, n_jobs=2,contamination=0.1)\n",
    "#     y_temp = clf._fit_predict(abnormal_data)\n",
    "#     score=clf.negative_outlier_factor_\n",
    "#     dist=-1*score\n",
    "#     d_nn5=dist\n",
    "\n",
    "    for r,t in zip(abnormal_data,range(len(abnormal_data))):                    ####knn\n",
    "            dist=[]\n",
    "\n",
    "            # dist_arg=[]\n",
    "            for o in abnormal_data:\n",
    "                    dist.append(np.linalg.norm(r-o))              \n",
    "                    \n",
    "            # dist_arg=(np.argsort(dist))\n",
    "            dist=np.sort(dist)\n",
    "            \n",
    "            d_nn1[k]=dist[1]\n",
    "            d_nn2[k]=dist[3]\n",
    "            d_nn3[k]=dist[5]\n",
    "            d_nn4[k]=dist[7]\n",
    "            d_nn5[k]=dist[11]\n",
    "\n",
    "            k+=1                                             ##knn\n",
    "            \n",
    "            \n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))   \n",
    "#     score_1=apprecons1(y_predtemp_1,lenth,bucket_size)    # for point\n",
    "#     score_2=apprecons1(y_predtemp_2,lenth,bucket_size)\n",
    "#     score_3=apprecons1(y_predtemp_3,lenth,bucket_size)\n",
    "\n",
    "#     w1=np.argsort(score_1)[::-1]     # for point\n",
    "#     w2=np.argsort(score_2)[::-1]\n",
    "#     w3=np.argsort(score_3)[::-1]\n",
    "\n",
    "    \n",
    "\n",
    "    w1=np.argsort(d_nn1)[::-1]                  # for seq\n",
    "    w2=np.argsort(d_nn2)[::-1]\n",
    "    w3=np.argsort(d_nn3)[::-1]\n",
    "    w4=np.argsort(d_nn4)[::-1]\n",
    "    w5=np.argsort(d_nn5)[::-1]\n",
    "# #     print(w2)\n",
    "\n",
    "    return  y_pred_arr1,d_nn1,y_pred_arr2,d_nn2,y_pred_arr3,d_nn3,y_pred_arr4,d_nn4,y_pred_arr5,d_nn5\n",
    "#     return  y_pred_arr1,score_1,y_pred_arr2,score_2,y_pred_arr3,score_3\n",
    "\n",
    "if __name__ == '__main__':\n",
    "              \n",
    "              for root, dirs, files in os.walk('/dataset/data_warp/'):\n",
    "#                      dirs=['DistalPhalanxOutlineCorrect','MiddlePhalanxOutlineCorrect','ProximalPhalanxOutlineCorrect','Earthquakes','PhalangesOutlinesCorrect','Strawberry','ToeSegmentation1','ToeSegmentation2','DodgerLoopGame','ECG5000','SyntheticControl','ECG200']\n",
    "                     dirs=['seq_data']\n",
    "#                      dirs=['DistalPhalanxOutlineCorrect','MiddlePhalanxOutlineCorrect','ProximalPhalanxOutlineCorrect','Earthquakes','PhalangesOutlinesCorrect','Strawberry','ToeSegmentation1','ToeSegmentation2','DodgerLoopGame','ECG5000','SyntheticControl','Wafer','HandOutlines']\n",
    "#                      dirs=['ECG200','Ford']\n",
    "#                      dirs=['artificial_seq']\n",
    "                     for dir in dirs:\n",
    "                        \n",
    "                        \n",
    "                        print(\"root=\",root)\n",
    "                        print(\"dir=\",dir)\n",
    "                        dir_=os.path.join(root,dir,\"ann/\")\n",
    "                        for root1,dir1,files in os.walk(dir_):\n",
    "                            print(files)\n",
    "#                             files=['Marotta_Valve_Tek_17_warp_embed.csv']\n",
    "#                             files=['Marotta_Valve_Tek_14_warp_embed.csv', 'chfdbchf15_warp_embed.csv','dutch_power_demand_warp_embed.csv','Patient_respiration_warp_embed.csv', 'Patient_respiration2_warp_embed.csv', 'Marotta_Valve_Tek_16_warp_embed.csv', 'ann_gun_CentroidA_1_warp_embed.csv','Marotta_Valve_Tek_17_warp_embed.csv']\n",
    "                            for file in files:\n",
    "                              \n",
    "                              \n",
    "                              if file.endswith(\"_warp_embed.csv\"): \n",
    "                                file_name = os.path.join(dir_,file)\n",
    "                                print(\"filename\",file_name)\n",
    "                                y_pred1, dist_1,y_pred2, dist_2,y_pred3, dist_3,y_pred4, dist_4,y_pred5, dist_5 = RunModel(file_name,root,dir,length_data[file.split('_warp_embed.csv')[-2]])\n",
    "                                \n",
    "                                abnormal_data,length,abnormal_label = ReadDataset_seq(os.path.join(root,dir,str(file.split('_warp_embed.csv')[0]+'.csv')))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_1)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_1)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_2)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_2)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_3)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_3)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_4)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_4)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_5)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_5)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "\n",
    "                                \n",
    "                            break\n",
    "                               \n",
    "                     break         \n",
    "                                \n",
    "                                \n",
    "                                 \n",
    "                                \n",
    " \n",
    "                                \n",
    "                                 \n",
    "                                "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NAB_datatowarp.ipynb",
   "provenance": [
    {
     "file_id": "1wYazsAo5mHQxJbaTNSulPUfnY5kJu0xT",
     "timestamp": 1576734667299
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
