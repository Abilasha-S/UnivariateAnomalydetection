{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install window-slider\n",
    "!pip install dtaidistance\n",
    "from dtaidistance import dtw\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import scipy.io\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import join, getsize\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score, precision_recall_curve, average_precision_score, cohen_kappa_score,confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from window_slider import Slider\n",
    "import sys\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "np.set_printoptions(suppress=True)\n",
    "from sklearn.neighbors import lof\n",
    "import glob\n",
    "from sklearn import metrics\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_data=dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadDataset_seq(_file_name, _normalize=True):\n",
    "\n",
    "    df = pd.read_csv(_file_name,header=None)\n",
    "    abnormal = df.values\n",
    "######change label position new and old\n",
    "###################################################################change#########################\n",
    "#     abnormal_data = abnormal[:,:-1] #change as position of class labels\n",
    "    abnormal_data = abnormal[:,1:]\n",
    "\n",
    "#     abnormal_label = abnormal[:,-1]\n",
    "    abnormal_label = abnormal[:,0]\n",
    "\n",
    "    print(\"anomaly\",(len(np.where(abnormal_label==-1)[0])))\n",
    "\n",
    "\n",
    "    # Normal = 1, Abnormal = -1\n",
    "    \n",
    "    return abnormal_data,len(abnormal_data),-1*abnormal_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "              for root, dirs, files in os.walk('/dataset/data_warp'):               \n",
    "                    dirs=['DistalPhalanxOutlineCorrect','MiddlePhalanxOutlineCorrect','ProximalPhalanxOutlineCorrect','Earthquakes','PhalangesOutlinesCorrect','Strawberry','ToeSegmentation1','ToeSegmentation2','DodgerLoopGame','ECG5000','SyntheticControl','ECG200','Wafer','HandOutlines']\n",
    "\n",
    "                    for dir in dirs:\n",
    "                        \n",
    "                        s_precision = []\n",
    "                        s_recall = []\n",
    "                        s_f1 = []\n",
    "                        s_roc_auc = []\n",
    "                        s_pr_auc = []\n",
    "                        s_cks = []\n",
    "                        \n",
    "                        print(\"root=\",root)\n",
    "                        print(\"dir=\",dir)\n",
    "                        \n",
    "                        for root1,dir1,files in os.walk(os.path.join(root,dir)):\n",
    "                             print(files)\n",
    "                             \n",
    "                             for file in files: \n",
    "                                if file.endswith('.csv'):\n",
    "                                    file_name = os.path.join(root,dir,file)\n",
    "                                    print(file_name)\n",
    "                                    abnormal_data,length,abnormal_label = ReadDataset_seq(file_name)\n",
    "                                    length_data[file.split('.csv')[-2]]=length\n",
    "                                    print(np.shape(abnormal_data))\n",
    "                                # print(np.shape(abnormal_label))\n",
    "                                    file_name1=os.path.join(os.path.join(root,dir),\"out_warp\",str(os.path.splitext(file)[0])+\"_warp.csv\")\n",
    "                                    np.savetxt(file_name1, abnormal_data, delimiter=',',fmt='%10.5f')\n",
    "#                                     file_name2=os.path.join(os.path.join(root,dir),\"out_warp\",str(os.path.splitext(file)[0])+\"_warp_label.csv\")\n",
    "#                                 # # file_name2=os.path.join(os.path.join(root,dir),\"out_warp\",str(os.path.splitext(file)[0])+\"_warp_revlabel.csv\")\n",
    "#                                     np.savetxt(file_name2, abnormal_label, delimiter=',',fmt='%d')\n",
    "                             break    \n",
    "                    break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadDataset_rep(_file_name,rep,root,dir, _normalize=True):\n",
    "\n",
    "    df = pd.read_csv(_file_name,header=None)\n",
    "    abnormal_data = df.values\n",
    "\n",
    "    # Normal = 1, Abnormal = -1\n",
    "    return abnormal_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RunModel(w,file,_file_name,root,dir,lenth):\n",
    "    rep=1\n",
    "\n",
    "    abnormal_data = ReadDataset_rep(_file_name,rep,root,dir) \n",
    "   \n",
    "    d_nn1=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn2=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn3=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn4=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn5=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "    k=0\n",
    "\n",
    "\n",
    "    for r,t in zip(abnormal_data,range(len(abnormal_data))):\n",
    "            dist=[] \n",
    "            for o in abnormal_data:\n",
    "\n",
    "                    dist.append(dtw.distance_fast(r,o,window=w))            \n",
    "            \n",
    "            dist=np.sort(dist)\n",
    "            \n",
    "            d_nn1[k]=dist[1]                     \n",
    "            d_nn2[k]=dist[5]\n",
    "            d_nn3[k]=dist[9]\n",
    "            d_nn4[k]=dist[15]\n",
    "            d_nn5[k]=dist[20]\n",
    "      \n",
    "            \n",
    "            k+=1                                                ##knn\n",
    "\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "  \n",
    "    return  d_nn1,d_nn2,d_nn3,d_nn4,d_nn5\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "              \n",
    "              for root, dirs, files in os.walk('/dataset/data_warp/'):\n",
    "                    i=0\n",
    "                    dirs=['DistalPhalanxOutlineCorrect','MiddlePhalanxOutlineCorrect','ProximalPhalanxOutlineCorrect','Earthquakes','PhalangesOutlinesCorrect','Strawberry','ToeSegmentation1','ToeSegmentation2','DodgerLoopGame','ECG5000','SyntheticControl','ECG200']\n",
    "                  \n",
    "                    for dir in dirs: \n",
    "                        print(\"root=\",root)\n",
    "                        print(\"dir=\",dir)\n",
    "                        dir_=os.path.join(root,dir,\"out_warp\")\n",
    "                      \n",
    "                        for root1,dir1,files in os.walk(dir_):\n",
    "                            print(files)\n",
    "\n",
    "                            for file in files:\n",
    "                              i=0\n",
    "                              best_mean=0\n",
    "                              if file.endswith(\"_warp.csv\"): \n",
    "                                file_name = os.path.join(dir_,file)\n",
    "                                print(\"filename\",file_name)\n",
    "                                abnormal_data,length,abnormal_label = ReadDataset_seq(os.path.join(root,dir,str(file.split('_warp.csv')[0]+'.csv')))\n",
    "\n",
    "                                dist_1, dist_2,dist_3, dist_4, dist_5 = RunModel(int(0.1*len(abnormal_data[0])),file,file_name,root,dir,length_data[file.split('_warp.csv')[-2]])                         \n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_1)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_1)\n",
    "#                                     auc_1=metrics.auc(fpr, tpr)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_2)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_2)\n",
    "#                                     auc_2=metrics.auc(fpr, tpr)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_3)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_3)\n",
    "#                                     auc_3=metrics.auc(fpr, tpr)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_4)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_4)\n",
    "#                                     auc_4=metrics.auc(fpr, tpr)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_5)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_5)\n",
    "#                                     auc_5=metrics.auc(fpr, tpr)\n",
    "#                                     mean_auc=np.mean([auc_1,auc_2,auc_3,auc_4,auc_5])\n",
    "#                                     if best_mean<auc_1:\n",
    "#                                             best_mean=auc_1\n",
    "#                                             best_w=w\n",
    "#                                 print(best_w)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                              \n",
    "                                  \n",
    "#                               \n",
    "#                             i=i+1\n",
    "                            break\n",
    "                    i=i+1          \n",
    "                    break         \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def RunModel(_file_name,root,dir,lenth):\n",
    "    rep=0\n",
    "\n",
    "    abnormal_data = ReadDataset_rep(_file_name,rep,root,dir) \n",
    "\n",
    "\n",
    "    d_nn1=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn2=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn3=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn4=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn5=[np.array(0) for i in range(len(abnormal_data))]\n",
    "\n",
    "    k=0\n",
    "    start_time = time.time()\n",
    "\n",
    "# ###########lof\n",
    "    clf = lof.LocalOutlierFactor(n_neighbors=1, n_jobs=2,contamination=0.5)\n",
    "    y_temp = clf._fit_predict(abnormal_data)\n",
    "    score=clf.negative_outlier_factor_\n",
    "    dist=-1*score\n",
    "    d_nn1=dist \n",
    "    clf = lof.LocalOutlierFactor(n_neighbors=3, n_jobs=2,contamination=0.5)\n",
    "    y_temp = clf._fit_predict(abnormal_data)\n",
    "    score=clf.negative_outlier_factor_\n",
    "    dist=-1*score \n",
    "    d_nn2=dist\n",
    "    clf = lof.LocalOutlierFactor(n_neighbors=5, n_jobs=2,contamination=0.5)\n",
    "    y_temp = clf._fit_predict(abnormal_data)\n",
    "    score=clf.negative_outlier_factor_\n",
    "    dist=-1*score\n",
    "    d_nn3=dist\n",
    "    clf = lof.LocalOutlierFactor(n_neighbors=7, n_jobs=2,contamination=0.5)\n",
    "    y_temp = clf._fit_predict(abnormal_data)\n",
    "    score=clf.negative_outlier_factor_\n",
    "    dist=-1*score \n",
    "    d_nn4=dist\n",
    "    clf = lof.LocalOutlierFactor(n_neighbors=11, n_jobs=2,contamination=0.5)\n",
    "    y_temp = clf._fit_predict(abnormal_data)\n",
    "    score=clf.negative_outlier_factor_\n",
    "    dist=-1*score\n",
    "    d_nn5=dist\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "    return  d_nn1,d_nn2,d_nn3,d_nn4,d_nn5\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "              \n",
    "              for root, dirs, files in os.walk('/dataset/data_warp/'):\n",
    "                   \n",
    "                    dirs=['DistalPhalanxOutlineCorrect','MiddlePhalanxOutlineCorrect','ProximalPhalanxOutlineCorrect','Earthquakes','PhalangesOutlinesCorrect','Strawberry','ToeSegmentation1','ToeSegmentation2','DodgerLoopGame','ECG5000','SyntheticControl','Wafer','HandOutlines']\n",
    "                   \n",
    "                    for dir in dirs:\n",
    "                        \n",
    "                        \n",
    "                        print(\"root=\",root)\n",
    "                        print(\"dir=\",dir)\n",
    "                        dir_=os.path.join(root,dir,\"out_warp\")\n",
    "                        for root1,dir1,files in os.walk(dir_):\n",
    "                            print(files)\n",
    "                            for file in files:\n",
    "                              \n",
    "                            \n",
    "                              if file.endswith(\"_warp.csv\"): \n",
    "                                file_name = os.path.join(dir_,file)\n",
    "                                print(\"filename\",file_name)\n",
    "                                dist_1,dist_2,dist_3,dist_4,dist_5= RunModel(file_name,root,dir,length_data[file.split('_warp.csv')[-2]])\n",
    "                                \n",
    "                                abnormal_data,length,abnormal_label = ReadDataset_seq(os.path.join(root,dir,str(file.split('_warp.csv')[0]+'.csv')))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_1)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_1)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_2)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_2)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_3)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_3)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_4)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_4)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_5)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_5)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                \n",
    "                             \n",
    "                            break\n",
    "                               \n",
    "                    break         \n",
    "                                \n",
    "                                \n",
    "                                 \n",
    "                                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_autoencoder(time_series, name, seed):\n",
    "        #encoder\n",
    "        # conv16@7\n",
    "        x = Conv1D(filters=16, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=initializers.he_normal(seed=seed), data_format='channels_last')(time_series)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        # conv16@5\n",
    "        x = Conv1D(filters=16, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=initializers.he_normal(seed=seed), data_format=\"channels_last\")(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        maxpool1_fanin = x._keras_shape[-2]\n",
    "        # maxpooling@3\n",
    "        x = MaxPool1D(pool_size=3, padding='same')(x)\n",
    "        # conv32@3\n",
    "        x = Conv1D(filters=32, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=initializers.he_normal(seed=seed), data_format=\"channels_last\")(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        # conv32@3\n",
    "        x = Conv1D(filters=32, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=initializers.he_normal(seed=seed), data_format=\"channels_last\")(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        # maxpooling@0.1L\n",
    "        maxpool2_fanin = x._keras_shape[-2]\n",
    "        pool_width = int(np.ceil(0.1 * maxpool2_fanin))\n",
    "        #signet pooling\n",
    "        x, mask = MaxPoolingWithArgmax1D(pool_size=pool_width, padding=\"same\")(x) #M,16\n",
    "        num_channels = x._keras_shape[-1]\n",
    "\n",
    "        x = Flatten()(x) # 16M length\n",
    "        num_nodes = x._keras_shape[-1]\n",
    "        # dense@rep_len\n",
    "        #rep = Dense(units=10, kernel_initializer=initializers.he_normal(seed=seed))(x)\n",
    "        rep = Dense(units=40, kernel_initializer=initializers.he_normal(seed=seed))(x)\n",
    "\n",
    "        # decoder\n",
    "        dense_upsample = Dense(units=num_nodes, kernel_initializer=initializers.he_normal(seed=seed))(rep) #160\n",
    "        x = Reshape((int(num_nodes/num_channels),num_channels))(dense_upsample) #M,16\n",
    "        # upsampling@0.1L\n",
    "        #x = UpSampling1D(size=pool_width)(x)\n",
    "        x = MaxUnpooling1D(pool_width)([x, mask])\n",
    "        upsample1_fanout = x._keras_shape[-2]\n",
    "        # crop excess from right end.\n",
    "        x = Cropping1D(cropping=(0, upsample1_fanout - maxpool2_fanin))(x) # L,16\n",
    "        # conv32@3\n",
    "        x = Conv1D(filters=32, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=initializers.he_normal(seed=seed), data_format=\"channels_last\")(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        # conv32@3\n",
    "        x = Conv1D(filters=16, kernel_size=3, strides=1, padding=\"same\", kernel_initializer=initializers.he_normal(seed=seed), data_format=\"channels_last\")(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        # upsampling@3\n",
    "        x = UpSampling1D(size=3)(x)\n",
    "        # Crop excess from the right end.\n",
    "        upsample2_fanout = x._keras_shape[-2]\n",
    "        x = Cropping1D(cropping=(0, upsample2_fanout - maxpool1_fanin))(x)\n",
    "        # conv16@5\n",
    "        x = Conv1D(filters=16, kernel_size=5, strides=1, padding=\"same\", kernel_initializer=initializers.he_normal(seed=seed), data_format=\"channels_last\")(x)\n",
    "        x = LeakyReLU(alpha=0.3)(x)\n",
    "        # conv16@7\n",
    "        x = Conv1D(filters=1, kernel_size=7, strides=1, padding=\"same\", kernel_initializer=initializers.he_normal(seed=seed), data_format=\"channels_last\")(x)\n",
    "        out = Lambda(lambda y:y, name=name)(x)\n",
    "\n",
    "        return rep, out\n",
    "\n",
    "\n",
    "\n",
    "def RunModel(_abnormal_data, _file_name):\n",
    "        model=[]\n",
    "        model_path=os.path.dirname(_file_name)\n",
    "        series_length=len(_abnormal_data[0])\n",
    "        time_series_1 = Input(shape=(series_length,1))\n",
    "        _abnormal_org=_abnormal_data\n",
    "        _abnormal_org=np.expand_dims(_abnormal_org,axis=-1)\n",
    "        random.shuffle(_abnormal_data)\n",
    "        _abnormal_data=np.expand_dims(_abnormal_data,axis=-1)\n",
    "        rep_layer_1, AE1_out = _create_autoencoder(time_series_1, 'AE1', 1)\n",
    "\n",
    "        model = Model(inputs=time_series_1, outputs=[AE1_out])\n",
    "\n",
    "        es = EarlyStopping(monitor='val_loss', mode='min', verbose=0, patience=100)\n",
    "        mc = ModelCheckpoint(str(model_path+'best_model.h5'), monitor='val_loss', mode='min', verbose=0, save_best_only=True)\n",
    "        losses = {\n",
    "                'AE1':'mean_squared_error',\n",
    "                }\n",
    "        model.compile(optimizer=Adam(lr=0.001), loss=losses)\n",
    "        model.fit(_abnormal_data,_abnormal_data, validation_split=0.2, epochs=2000,verbose=0,callbacks=[es,mc])\n",
    "\n",
    "        model1=load_model(str(model_path+'best_model.h5'),custom_objects={\"MaxPoolingWithArgmax1D\": layers.MaxPoolingWithArgmax1D,\"MaxUnpooling1D\": layers.MaxUnpooling1D})\n",
    "        inputs = model1.get_layer(index=0).input\n",
    "        output1 = model1.get_layer(index=12).output\n",
    "        output2=model1.get_layer(index=26).output\n",
    "        \n",
    "        outputs=[output1, output2]\n",
    "        intermediate_model = Model(inputs=inputs, outputs=outputs)\n",
    "        print(intermediate_model.summary())\n",
    "        rep, recons=intermediate_model.predict(_abnormal_org)\n",
    "        recons=np.squeeze(recons)\n",
    "        del model\n",
    "        del intermediate_model\n",
    "        return rep, recons\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "\n",
    "    for root, dirs, _ in os.walk('/dataset/data_warp/'):\n",
    "                dirs=['DistalPhalanxOutlineCorrect','MiddlePhalanxOutlineCorrect','ProximalPhalanxOutlineCorrect','Earthquakes','PhalangesOutlinesCorrect','Strawberry','ToeSegmentation1','ToeSegmentation2','DodgerLoopGame','ECG5000','SyntheticControl','Wafer','HandOutlines']\n",
    "\n",
    "      \n",
    "                for dir in dirs:\n",
    "                    print(\"root=\",root)\n",
    "                    print(\"dir=\",dir)\n",
    "                    dir_=os.path.join(root,dir,\"out_warp\")\n",
    "                    for root1,dir1,files in os.walk(dir_):\n",
    "\n",
    "                           for file in files:\n",
    "\n",
    "                              if file.endswith(\"_warp.csv\"):\n",
    "                              # if file.endswith(\".csv\"):\n",
    "                                file_name = os.path.join(dir_,file)\n",
    "                                # file_name = os.path.join(os.path.join(root,dir),file)\n",
    "#                                 print(\"filename\",file_name)\n",
    "\n",
    "                                final_error = []\n",
    "                                final_recons=[]\n",
    "                                abnormal_data = ReadDataset_seq(file_name)\n",
    "\n",
    "\n",
    "                                embed, recons = RunModel(np.array(abnormal_data),_file_name=file_name)\n",
    "\n",
    "                                file_name1=os.path.join(os.path.join(root,dir),\"ann/\",str(os.path.splitext(file)[0])+\"_recons.csv\")\n",
    "                                print(file_name1)\n",
    "                                np.savetxt(file_name1,recons, delimiter=',',fmt='%10.5f')\n",
    "\n",
    "                                file_name1=os.path.join(os.path.join(root,dir),\"ann/\",str(os.path.splitext(file)[0])+\"_embed.csv\")\n",
    "                                print(file_name1)\n",
    "                                np.savetxt(file_name1, embed, delimiter=',',fmt='%10.5f')\n",
    "\n",
    "\n",
    "                            break\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def RunModel(_file_name,root,dir,lenth):\n",
    "    rep=1\n",
    "    abnormal_data = ReadDataset_rep(_file_name,rep,root,dir) \n",
    "\n",
    "    d_nn1=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn2=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn3=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn4=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn5=[np.array(0) for i in range(len(abnormal_data))]\n",
    "\n",
    "    k=0\n",
    "    start_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "    for r,t in zip(abnormal_data,range(len(abnormal_data))):                    ####knn\n",
    "            dist=[]\n",
    "\n",
    "            # dist_arg=[]\n",
    "            for o in abnormal_data:\n",
    "                    dist.append(np.linalg.norm(r-o))              \n",
    "                    \n",
    "            # dist_arg=(np.argsort(dist))\n",
    "            dist=np.sort(dist)\n",
    "            \n",
    "            d_nn1[k]=dist[1]\n",
    "            d_nn2[k]=dist[3]\n",
    "            d_nn3[k]=dist[5]\n",
    "            d_nn4[k]=dist[7]\n",
    "            d_nn5[k]=dist[11]\n",
    "\n",
    "\n",
    "            k+=1                                             ##knn\n",
    "            \n",
    "            \n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))   \n",
    "\n",
    "\n",
    "    return  d_nn1,d_nn2,d_nn3,d_nn4,d_nn5\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "              \n",
    "              for root, dirs, files in os.walk('/datasset/data_warp/'):\n",
    "                     dirs=['DistalPhalanxOutlineCorrect','MiddlePhalanxOutlineCorrect','ProximalPhalanxOutlineCorrect','Earthquakes','PhalangesOutlinesCorrect','Strawberry','ToeSegmentation1','ToeSegmentation2','DodgerLoopGame','ECG5000','SyntheticControl','ECG200']\n",
    "\n",
    "                     for dir in dirs:\n",
    "                        \n",
    "                        \n",
    "                        print(\"root=\",root)\n",
    "                        print(\"dir=\",dir)\n",
    "                        dir_=os.path.join(root,dir,\"ann/\")\n",
    "                        for root1,dir1,files in os.walk(dir_):\n",
    "                            print(files)\n",
    "                            for file in files:\n",
    "                              \n",
    "                              \n",
    "                              if file.endswith(\"_warp_embed.csv\"): \n",
    "                                file_name = os.path.join(dir_,file)\n",
    "                                print(\"filename\",file_name)\n",
    "                                dist_1,dist_2,dist_3,dist_4,dist_5 = RunModel(file_name,root,dir,length_data[file.split('_warp_embed.csv')[-2]])\n",
    "                                \n",
    "                                abnormal_data,length,abnormal_label = ReadDataset_seq(os.path.join(root,dir,str(file.split('_warp_embed.csv')[0]+'.csv')))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_1)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_1)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_2)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_2)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_3)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_3)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_4)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_4)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_5)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_5)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                            \n",
    "                            break\n",
    "                               \n",
    "                     break         \n",
    "                                \n",
    "                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sub-seq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadDataset_roll(_file_name_annot,anom_size):\n",
    "    bucket_size=50\n",
    "    base=os.path.basename(_file_name_annot) \n",
    "    base_name=base.split('Annotations_')[1]\n",
    "    file_name=os.path.join(os.path.dirname(_file_name_annot),base_name)\n",
    "    label_new=[]\n",
    "    value=[]\n",
    "    with open(file_name) as infile:\n",
    "        value = [float(line.strip('\\n')) for line in infile if line]\n",
    "    abnormal_data=np.array(value)\n",
    "    print(\"org len\",len(abnormal_data))\n",
    "    \n",
    "    with open(_file_name_annot) as infile:\n",
    "            indice = [int(line.strip('\\n')) for line in infile if line]\n",
    "        \n",
    "    abnormal_label=np.ones(len(abnormal_data))\n",
    "\n",
    "    for i in indice:\n",
    "                abnormal_label[i:i+anom_size]=-1\n",
    "       \n",
    "#     bucket_size=anom_size+20  \n",
    "    abnorm_data=np.empty((0,bucket_size),float)\n",
    "    abnorm_label=np.empty((0,bucket_size),float)\n",
    "    overlap_count = 0\n",
    "    slider = Slider(bucket_size,overlap_count)\n",
    "    slider1 = Slider(bucket_size,overlap_count)\n",
    "    slider.fit(abnormal_data) \n",
    "    slider1.fit(abnormal_label)\n",
    "    plt.plot(abnormal_data)\n",
    "    plt.show()\n",
    "    plt.plot(abnormal_label)\n",
    "    plt.show()\n",
    "    while True:\n",
    "        window_data1 = slider.slide() \n",
    "        window_label1= slider1.slide()\n",
    "        if  (len(window_data1)<bucket_size) and len(window_data1)>bucket_size/2:\n",
    "          window_data=np.pad(window_data1,[0,(bucket_size-len(window_data1))])\n",
    "          window_label=np.pad(window_label1,[0,(bucket_size-len(window_label1))])\n",
    "          abnorm_data= np.append(abnorm_data,[window_data], axis=0)\n",
    "          abnorm_label= np.append(abnorm_label,[window_label], axis=0)\n",
    "          break\n",
    "        else:\n",
    "          if len(window_data1)==0 or len(window_data1)<bucket_size/2:\n",
    "                break\n",
    "          abnorm_data=np.append(abnorm_data,[window_data1], axis=0)\n",
    "          abnorm_label=np.append(abnorm_label,[window_label1], axis=0)\n",
    "            \n",
    "    label=np.sign(np.sum(abnorm_label,axis=1))\n",
    "    combined_data=np.column_stack((label,abnorm_data))\n",
    "\n",
    "    return combined_data, anom_count\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "              anom_track=dict()\n",
    "              for root, dirs, files in os.walk('/dataset/discorddataset/'):\n",
    "                    for dir in dirs:                        \n",
    "                        print(\"root=\",root)\n",
    "                        print(\"dir=\",dir)  \n",
    "                        anom_dict={'Marotta_Valve_Tek_14': 128,'Marotta_Valve_Tek_17': 128, 'chfdbchf15': 200, 'ann_gun_CentroidA_1': 150, 'Patient_respiration2': 150, 'Patient_respiration': 100, 'Marotta_Valve_Tek_16': 128, 'dutch_power_demand': 800}\n",
    "                        for file in glob.glob(str(root+dir+'/Annotations_*.txt')): \n",
    "                                        base=os.path.basename(file)\n",
    "                                        base=base.split('Annotations_')[1]\n",
    "                                        base=base.split('.txt')[0]\n",
    "                                        print(file)\n",
    "                                        abnormal_data, a_count = ReadDataset_roll(file,anom_dict[base])\n",
    "                                        anom_track[base]=a_count\n",
    "                                        print(np.shape(abnormal_data))\n",
    "                                        np.savetxt(os.path.join(root,dir,str(base+'.csv')), abnormal_data, delimiter=',',fmt='%10.5f')\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def RunModel(w,file,_file_name,root,dir,lenth):\n",
    "    rep=1\n",
    "\n",
    "    \n",
    "    \n",
    "    d_nn1=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn2=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn3=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn4=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn5=[np.array(0) for i in range(len(abnormal_data))]\n",
    "\n",
    "    \n",
    "    start_time = time.time()\n",
    "    k=0\n",
    "\n",
    "    for r,t in zip(abnormal_data,range(len(abnormal_data))):\n",
    "            dist=[]  \n",
    "            for o in abnormal_data:\n",
    "                dist.append(dtw.distance(r,o,window=w))  \n",
    "            \n",
    "\n",
    "            dist=np.sort(dist)\n",
    "            \n",
    "            d_nn1[k]=dist[1]                     \n",
    "            d_nn2[k]=dist[5]\n",
    "            d_nn3[k]=dist[9]\n",
    "            d_nn4[k]=dist[15]\n",
    "            d_nn5[k]=dist[20]\n",
    "  \n",
    "            k+=1                                                ##knn\n",
    "            \n",
    "\n",
    "\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    " \n",
    "    return  y_pred_arr1,d_nn1,y_pred_arr2,d_nn2,y_pred_arr3,d_nn3,y_pred_arr4,d_nn4,y_pred_arr5,d_nn5\n",
    "\n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "              \n",
    "              for root, dirs, files in os.walk('/dataset/data_warp/'):\n",
    "                    i=0\n",
    "                    dirs=['seq_data']\n",
    "                    for dir in dirs: \n",
    "                        print(\"root=\",root)\n",
    "                        print(\"dir=\",dir)\n",
    "                        dir_=os.path.join(root,dir,\"out_warp\")\n",
    "                      \n",
    "                        for root1,dir1,files in os.walk(dir_):\n",
    "                            print(files)\n",
    "#                             w=[7,13,20,20,16,8,8,9]\n",
    "                            files=['dutch_power_demand_warp.csv','Marotta_Valve_Tek_14_warp.csv','ann_gun_CentroidA_1_warp.csv',  'chfdbchf15_warp.csv','Patient_respiration2_warp.csv', 'Patient_respiration_warp.csv', 'Marotta_Valve_Tek_16_warp.csv',  'Marotta_Valve_Tek_17_warp.csv']\n",
    "                            for file in files:\n",
    "                              i=0\n",
    "                              best_mean=0\n",
    "                              if file.endswith(\"_warp.csv\"): \n",
    "                                file_name = os.path.join(dir_,file)\n",
    "                                print(\"filename\",file_name)\n",
    "                                abnormal_data,length,abnormal_label = ReadDataset_seq(os.path.join(root,dir,str(file.split('_warp.csv')[0]+'.csv')))\n",
    "\n",
    "                                y_pred1, dist_1,y_pred2, dist_2,y_pred3, dist_3,y_pred4, dist_4,y_pred5, dist_5 = RunModel(int(0.1*len(abnormal_data[0])),file,file_name,root,dir,length_data[file.split('_warp.csv')[-2]])                         \n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_1)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_1)\n",
    "#                                     auc_1=metrics.auc(fpr, tpr)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_2)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_2)\n",
    "#                                     auc_2=metrics.auc(fpr, tpr)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_3)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_3)\n",
    "#                                     auc_3=metrics.auc(fpr, tpr)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_4)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_4)\n",
    "#                                     auc_4=metrics.auc(fpr, tpr)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_5)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_5)\n",
    "#                                     auc_5=metrics.auc(fpr, tpr)\n",
    "#                                     mean_auc=np.mean([auc_1,auc_2,auc_3,auc_4,auc_5])\n",
    "#                                     if best_mean<auc_1:\n",
    "#                                             best_mean=auc_1\n",
    "#                                             best_w=w\n",
    "#                                 print(best_w)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "\n",
    "                                  \n",
    "#                       \n",
    "                            i=i+1\n",
    "                            break         \n",
    "                    break         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
