{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "executionInfo": {
     "elapsed": 8524,
     "status": "ok",
     "timestamp": 1584095859508,
     "user": {
      "displayName": "Abilasha S",
      "photoUrl": "",
      "userId": "17146042062219803973"
     },
     "user_tz": -330
    },
    "id": "agVKCjJroVz7",
    "outputId": "7489b462-470b-4348-8705-b16d701561af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: window-slider in /home/abilasha/anaconda3/lib/python3.7/site-packages (0.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.1; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/abilasha/anaconda3/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install window-slider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "executionInfo": {
     "elapsed": 10006,
     "status": "ok",
     "timestamp": 1584095861385,
     "user": {
      "displayName": "Abilasha S",
      "photoUrl": "",
      "userId": "17146042062219803973"
     },
     "user_tz": -330
    },
    "id": "s6AHqd6ujph4",
    "outputId": "09b33fcd-c7ce-456c-f60f-1b47ff45dd93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/abilasha/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import scipy.io\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import join, getsize\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score, precision_recall_curve, average_precision_score, cohen_kappa_score,confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from window_slider import Slider\n",
    "import os\n",
    "import pathlib\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import pathlib\n",
    "import numpy as np\n",
    "np.set_printoptions(suppress=True)\n",
    "import pandas as pd\n",
    "import json\n",
    "import scipy.io\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import join, getsize\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score, precision_recall_curve, average_precision_score, cohen_kappa_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQFEt9bxlF8U"
   },
   "outputs": [],
   "source": [
    "bucket_size = 25\n",
    "length_data=dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vEQ8K0dChT22"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ReadNABDataset(_file_name, _normalize=True):\n",
    "    with open('/dataset/labels/combined_windows.json') as data_file:\n",
    "        json_label = json.load(data_file)\n",
    "    abnormal = pd.read_csv(_file_name, header=0, index_col=0)\n",
    "    abnormal['label'] = -1\n",
    "    # abnormal['label'] = 1\n",
    "    dir_name=\"/dataset/data_warp\"\n",
    "    list_windows = json_label.get(os.path.relpath(_file_name,dir_name))\n",
    "    for window in list_windows:\n",
    "        start = window[0]\n",
    "        end = window[1]\n",
    "        abnormal.loc[start:end, 'label'] = 1\n",
    "        # abnormal.loc[start:end, 'label'] = -1\n",
    "\n",
    "    abnormal_data = abnormal['value'].as_matrix()\n",
    "    \n",
    "    # abnormal_preprocessing_data = np.reshape(abnormal_preprocessing_data, (abnormal_preprocessing_data.shape[0], 1))\n",
    "    abnormal_label = abnormal['label'].as_matrix()\n",
    "\n",
    "    print(\"org len\",len(abnormal_data))\n",
    "    print(\"anomaly\",(len(np.where(abnormal_label==1)[0])))\n",
    "    # abnormal_data = np.expand_dims(abnormal_data, axis=1)\n",
    "    # abnormal_label = np.expand_dims(abnormal_label, axis=1)\n",
    "\n",
    "    \n",
    "    \n",
    "    # abnorm_data=np.zeros((len(abnormal_data),bucket_size)) \n",
    "    abnorm_data=np.empty((0,bucket_size),float)\n",
    "    overlap_count =24\n",
    "    # overlap_count = 0\n",
    "    slider = Slider(bucket_size,overlap_count)\n",
    "    slider.fit(abnormal_data)\n",
    "          \n",
    "    while True:\n",
    "        window_data1 = slider.slide()\n",
    "        \n",
    "        # do your stuff\n",
    "        if not (len(window_data1)==bucket_size):\n",
    "          # print(\"data\",np.shape(window_data1))\n",
    "          window_data=np.pad(window_data1,[0,(bucket_size-len(window_data1))],mode='constant')\n",
    "          abnorm_data= np.append(abnorm_data,[window_data], axis=0)\n",
    "          # print(np.shape(abnorm_data))\n",
    "          break\n",
    "        else:\n",
    "          # abnorm_data[i]=window_data1\n",
    "          abnorm_data=np.append(abnorm_data,[window_data1], axis=0)\n",
    "        \n",
    "        \n",
    "\n",
    "    # abnormal_data = np.expand_dims(abnorm_data, axis=1)\n",
    "\n",
    "\n",
    "    # abnorm_label=np.zeros((len(abnormal_label),bucket_size)) \n",
    "\n",
    "    #-------------------------------------------------------------------------------------\n",
    "    abnorm_label=np.empty((0,bucket_size),float)\n",
    "    slider = Slider(bucket_size,overlap_count)\n",
    "    slider.fit(abnormal_label)\n",
    "           \n",
    "    while True:\n",
    "        window_label1 = slider.slide()\n",
    "        \n",
    "        # do your stuff\n",
    "        if not (len(window_label1)==bucket_size):\n",
    "          # print(np.shape(window_label1))\n",
    "          window_label=np.pad(window_label1,[0,(bucket_size-len(window_label1))],mode='constant',constant_values=-1)\n",
    "          abnorm_label=np.append(abnorm_label,[window_label], axis=0)\n",
    "          # print(np.shape(abnorm_label[i]))\n",
    "          break\n",
    "        else:\n",
    "          # abnorm_label[i]=window_label1\n",
    "          abnorm_label=np.append(abnorm_label,[window_label1], axis=0)\n",
    "\n",
    "    label=-1*np.ones(len(abnorm_label)) \n",
    "    # label=np.ones(len(abnorm_label))   \n",
    "    for i in range(len(abnorm_label)):\n",
    "      for j in abnorm_label[i]:\n",
    "        if j==1:\n",
    "          label[i]=1\n",
    "          # label[i]=-1\n",
    "          break\n",
    "    print(\"anom rep\",len(np.where(label==1)[0]))\n",
    "    #-------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # abnormal_label = np.expand_dims(abnorm_label, axis=1)\n",
    "\n",
    "    # print((abnorm_data))\n",
    "\n",
    "    # if _normalize==True:\n",
    "    #     scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    #     abnormal_data = scaler.fit_transform(abnormal_data)\n",
    "\n",
    "    # Normal = 1, Abnormal = -1\n",
    "    # return abnorm_data, label\n",
    "    return abnorm_data,len(abnormal_data),abnormal_label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkNBwilJSHRQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ReadDataset_withlab(_file_name, _normalize=True):\n",
    " \n",
    "    abnormal = pd.read_csv(_file_name)\n",
    "    abnormal=abnormal.values\n",
    "    \n",
    "\n",
    "\n",
    "    abnormal_data = abnormal[:,0]\n",
    "    \n",
    "    # abnormal_preprocessing_data = np.reshape(abnormal_preprocessing_data, (abnormal_preprocessing_data.shape[0], 1))\n",
    "    abnormal_label = abnormal[:,1]\n",
    "\n",
    "    print(\"org len\",len(abnormal_data))\n",
    "    print(\"anomaly\",(len(np.where(abnormal_label==1)[0])))\n",
    "\n",
    "    abnorm_data=np.empty((0,bucket_size),float)\n",
    "    overlap_count =24\n",
    "    # overlap_count = 0\n",
    "    slider = Slider(bucket_size,overlap_count)\n",
    "    slider.fit(abnormal_data)\n",
    "          \n",
    "    while True:\n",
    "        window_data1 = slider.slide()\n",
    "        \n",
    "        # do your stuff\n",
    "        if not (len(window_data1)==bucket_size):\n",
    "          # print(\"data\",np.shape(window_data1))\n",
    "          window_data=np.pad(window_data1,[0,(bucket_size-len(window_data1))],mode='constant')\n",
    "          abnorm_data= np.append(abnorm_data,[window_data], axis=0)\n",
    "          # print(np.shape(abnorm_data))\n",
    "          break\n",
    "        else:\n",
    "          # abnorm_data[i]=window_data1\n",
    "          abnorm_data=np.append(abnorm_data,[window_data1], axis=0)\n",
    "        \n",
    "        \n",
    "\n",
    "    # abnormal_data = np.expand_dims(abnorm_data, axis=1)\n",
    "\n",
    "\n",
    "    # abnorm_label=np.zeros((len(abnormal_label),bucket_size)) \n",
    "\n",
    "    #-------------------------------------------------------------------------------------\n",
    "    abnorm_label=np.empty((0,bucket_size),float)\n",
    "    slider = Slider(bucket_size,overlap_count)\n",
    "    slider.fit(abnormal_label)\n",
    "           \n",
    "    while True:\n",
    "        window_label1 = slider.slide()\n",
    "        \n",
    "        # do your stuff\n",
    "        if not (len(window_label1)==bucket_size):\n",
    "          # print(np.shape(window_label1))\n",
    "          window_label=np.pad(window_label1,[0,(bucket_size-len(window_label1))],mode='constant',constant_values=-1)\n",
    "          abnorm_label=np.append(abnorm_label,[window_label], axis=0)\n",
    "          # print(np.shape(abnorm_label[i]))\n",
    "          break\n",
    "        else:\n",
    "          # abnorm_label[i]=window_label1\n",
    "          abnorm_label=np.append(abnorm_label,[window_label1], axis=0)\n",
    "\n",
    "    label=-1*np.ones(len(abnorm_label)) \n",
    "    # label=np.ones(len(abnorm_label))   \n",
    "    for i in range(len(abnorm_label)):\n",
    "      for j in abnorm_label[i]:\n",
    "        if j==1:\n",
    "          label[i]=1\n",
    "          # label[i]=-1\n",
    "          break\n",
    "    print(\"anom rep\",len(np.where(label==1)[0]))\n",
    "    #-------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    # abnormal_label = np.expand_dims(abnorm_label, axis=1)\n",
    "\n",
    "    # print((abnorm_data))\n",
    "\n",
    "    # if _normalize==True:\n",
    "    #     scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    #     abnormal_data = scaler.fit_transform(abnormal_data)\n",
    "\n",
    "    # Normal = 1, Abnormal = -1\n",
    "    # return abnorm_data, label\n",
    "    return abnorm_data,len(abnormal_data),abnormal_label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "executionInfo": {
     "elapsed": 1609,
     "status": "ok",
     "timestamp": 1584105250032,
     "user": {
      "displayName": "Abilasha S",
      "photoUrl": "",
      "userId": "17146042062219803973"
     },
     "user_tz": -330
    },
    "id": "dULBUi6dhcW8",
    "outputId": "0471d50b-87f9-4662-9222-c86e38351cc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root= /home/abilasha/Downloads/NAB/data_warp\n",
      "dir= realKnownCause\n",
      "['cpu_utilization_asg_misconfiguration.csv']\n",
      "/home/abilasha/Downloads/NAB/data_warp/realKnownCause/cpu_utilization_asg_misconfiguration.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abilasha/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/abilasha/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "org len 18050\n",
      "anomaly 1498\n",
      "anom rep 1499\n",
      "(18027, 25)\n",
      "(18050,)\n",
      "root= /home/abilasha/Downloads/NAB/data_warp\n",
      "dir= realAWSCloudwatch\n",
      "['ec2_cpu_utilization_53ea38.csv', 'ec2_cpu_utilization_ac20cd.csv', 'ec2_cpu_utilization_fe7f93.csv', 'rds_cpu_utilization_cc0c53.csv', 'ec2_network_in_257a54.csv', 'ec2_cpu_utilization_24ae8d.csv', 'elb_request_count_8c0756.csv', 'ec2_cpu_utilization_825cc2.csv', 'ec2_disk_write_bytes_1ef3de.csv', 'ec2_cpu_utilization_77c1ca.csv', 'iio_us-east-1_i-a2eb1cd9_NetworkIn.csv', 'grok_asg_anomaly.csv']\n",
      "/home/abilasha/Downloads/NAB/data_warp/realAWSCloudwatch/ec2_cpu_utilization_53ea38.csv\n",
      "org len 4032\n",
      "anomaly 400\n",
      "anom rep 448\n",
      "(4009, 25)\n",
      "(4032,)\n",
      "/home/abilasha/Downloads/NAB/data_warp/realAWSCloudwatch/ec2_cpu_utilization_ac20cd.csv\n",
      "org len 4032\n",
      "anomaly 402\n",
      "anom rep 426\n",
      "(4009, 25)\n",
      "(4032,)\n",
      "/home/abilasha/Downloads/NAB/data_warp/realAWSCloudwatch/ec2_cpu_utilization_fe7f93.csv\n",
      "org len 4032\n",
      "anomaly 402\n",
      "anom rep 474\n",
      "(4009, 25)\n",
      "(4032,)\n",
      "/home/abilasha/Downloads/NAB/data_warp/realAWSCloudwatch/rds_cpu_utilization_cc0c53.csv\n",
      "org len 4032\n",
      "anomaly 400\n",
      "anom rep 448\n",
      "(4009, 25)\n",
      "(4032,)\n",
      "/home/abilasha/Downloads/NAB/data_warp/realAWSCloudwatch/ec2_network_in_257a54.csv\n",
      "org len 4032\n",
      "anomaly 402\n",
      "anom rep 426\n",
      "(4009, 25)\n",
      "(4032,)\n",
      "/home/abilasha/Downloads/NAB/data_warp/realAWSCloudwatch/ec2_cpu_utilization_24ae8d.csv\n",
      "org len 4032\n",
      "anomaly 400\n",
      "anom rep 448\n",
      "(4009, 25)\n",
      "(4032,)\n",
      "/home/abilasha/Downloads/NAB/data_warp/realAWSCloudwatch/elb_request_count_8c0756.csv\n",
      "org len 4032\n",
      "anomaly 400\n",
      "anom rep 448\n",
      "(4009, 25)\n",
      "(4032,)\n",
      "/home/abilasha/Downloads/NAB/data_warp/realAWSCloudwatch/ec2_cpu_utilization_825cc2.csv\n",
      "org len 4032\n",
      "anomaly 342\n",
      "anom rep 366\n",
      "(4009, 25)\n",
      "(4032,)\n",
      "/home/abilasha/Downloads/NAB/data_warp/realAWSCloudwatch/ec2_disk_write_bytes_1ef3de.csv\n",
      "org len 4730\n",
      "anomaly 472\n",
      "anom rep 496\n",
      "(4707, 25)\n",
      "(4730,)\n",
      "/home/abilasha/Downloads/NAB/data_warp/realAWSCloudwatch/ec2_cpu_utilization_77c1ca.csv\n",
      "org len 4032\n",
      "anomaly 402\n",
      "anom rep 426\n",
      "(4009, 25)\n",
      "(4032,)\n",
      "/home/abilasha/Downloads/NAB/data_warp/realAWSCloudwatch/iio_us-east-1_i-a2eb1cd9_NetworkIn.csv\n",
      "org len 1243\n",
      "anomaly 124\n",
      "anom rep 172\n",
      "(1220, 25)\n",
      "(1243,)\n",
      "/home/abilasha/Downloads/NAB/data_warp/realAWSCloudwatch/grok_asg_anomaly.csv\n",
      "org len 4621\n",
      "anomaly 462\n",
      "anom rep 534\n",
      "(4598, 25)\n",
      "(4621,)\n",
      "root= /home/abilasha/Downloads/NAB/data_warp\n",
      "dir= realAdExchange\n",
      "['exchange-3_cpm_results.csv']\n",
      "/home/abilasha/Downloads/NAB/data_warp/realAdExchange/exchange-3_cpm_results.csv\n",
      "org len 1538\n",
      "anomaly 152\n",
      "anom rep 176\n",
      "(1515, 25)\n",
      "(1538,)\n",
      "root= /home/abilasha/Downloads/NAB/data_warp\n",
      "dir= realTraffic\n",
      "['TravelTime_387.csv', 'TravelTime_451.csv']\n",
      "/home/abilasha/Downloads/NAB/data_warp/realTraffic/TravelTime_387.csv\n",
      "org len 2500\n",
      "anomaly 246\n",
      "anom rep 318\n",
      "(2477, 25)\n",
      "(2500,)\n",
      "/home/abilasha/Downloads/NAB/data_warp/realTraffic/TravelTime_451.csv\n",
      "org len 2162\n",
      "anomaly 216\n",
      "anom rep 240\n",
      "(2139, 25)\n",
      "(2162,)\n",
      "root= /home/abilasha/Downloads/NAB/data_warp\n",
      "dir= realTweets\n",
      "['Twitter_volume_UPS.csv', 'Twitter_volume_IBM.csv', 'Twitter_volume_AAPL.csv', 'Twitter_volume_AMZN.csv', 'Twitter_volume_PFE.csv', 'Twitter_volume_GOOG.csv']\n",
      "/home/abilasha/Downloads/NAB/data_warp/realTweets/Twitter_volume_UPS.csv\n",
      "org len 15866\n",
      "anomaly 1580\n",
      "anom rep 1699\n",
      "(15843, 25)\n",
      "(15866,)\n",
      "/home/abilasha/Downloads/NAB/data_warp/realTweets/Twitter_volume_IBM.csv\n",
      "org len 15893\n",
      "anomaly 1588\n",
      "anom rep 1636\n",
      "(15870, 25)\n",
      "(15893,)\n",
      "/home/abilasha/Downloads/NAB/data_warp/realTweets/Twitter_volume_AAPL.csv\n",
      "org len 15902\n",
      "anomaly 1584\n",
      "anom rep 1680\n",
      "(15879, 25)\n",
      "(15902,)\n",
      "/home/abilasha/Downloads/NAB/data_warp/realTweets/Twitter_volume_AMZN.csv\n",
      "org len 15831\n",
      "anomaly 1576\n",
      "anom rep 1672\n",
      "(15808, 25)\n",
      "(15831,)\n",
      "/home/abilasha/Downloads/NAB/data_warp/realTweets/Twitter_volume_PFE.csv\n",
      "org len 15858\n",
      "anomaly 1584\n",
      "anom rep 1680\n",
      "(15835, 25)\n",
      "(15858,)\n",
      "/home/abilasha/Downloads/NAB/data_warp/realTweets/Twitter_volume_GOOG.csv\n",
      "org len 15842\n",
      "anomaly 1429\n",
      "anom rep 1501\n",
      "(15819, 25)\n",
      "(15842,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "              for root, dirs, files in os.walk('/dataset/data_warp'):\n",
    "#                     dirs=['artificial']\n",
    "                    dirs=['realKnownCause','realAWSCloudwatch','realAdExchange','realTraffic','realTweets']\n",
    "                    for dir in dirs:\n",
    "                        \n",
    "                        s_precision = []\n",
    "                        s_recall = []\n",
    "                        s_f1 = []\n",
    "                        s_roc_auc = []\n",
    "                        s_pr_auc = []\n",
    "                        s_cks = []\n",
    "                        \n",
    "                        print(\"root=\",root)\n",
    "                        print(\"dir=\",dir)\n",
    "                        \n",
    "                        for root1,dir1,files in os.walk(os.path.join(root,dir)):\n",
    "                             print(files)\n",
    "#                              files=['artificial_point.csv']\n",
    "                             for file in files:   \n",
    "                                file_name = os.path.join(root,dir,file)\n",
    "                                print(file_name)\n",
    "                                \n",
    "                                abnormal_data,length,abnormal_label = ReadNABDataset(file_name)\n",
    "#                                 abnormal_data,length,abnormal_label = ReadDataset_withlab(file_name)\n",
    "                                length_data[file.split('.csv')[-2]]=length\n",
    "                                print(np.shape(abnormal_data))\n",
    "                                print(np.shape(abnormal_label))\n",
    "                                file_name1=os.path.join(os.path.join(root,dir),\"out_warp\",str(os.path.splitext(file)[0])+\"_warp.csv\")\n",
    "                                np.savetxt(file_name1, abnormal_data, delimiter=',',fmt='%10.5f')\n",
    "#                                 file_name2=os.path.join(os.path.join(root,dir),\"out_warp\",str(os.path.splitext(file)[0])+\"_warp_label.csv\")\n",
    "#                                 # file_name2=os.path.join(os.path.join(root,dir),\"out_warp\",str(os.path.splitext(file)[0])+\"_warp_revlabel.csv\")\n",
    "#                                 np.savetxt(file_name2, abnormal_label, delimiter=',',fmt='%d')\n",
    "                             break    \n",
    "                    break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1248,
     "status": "ok",
     "timestamp": 1584105254011,
     "user": {
      "displayName": "Abilasha S",
      "photoUrl": "",
      "userId": "17146042062219803973"
     },
     "user_tz": -330
    },
    "id": "H_sIbBefyRdq",
    "outputId": "ce8f7458-82ee-4243-8383-8f8eda29839d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cpu_utilization_asg_misconfiguration': 18050, 'ec2_cpu_utilization_53ea38': 4032, 'ec2_cpu_utilization_ac20cd': 4032, 'ec2_cpu_utilization_fe7f93': 4032, 'rds_cpu_utilization_cc0c53': 4032, 'ec2_network_in_257a54': 4032, 'ec2_cpu_utilization_24ae8d': 4032, 'elb_request_count_8c0756': 4032, 'ec2_cpu_utilization_825cc2': 4032, 'ec2_disk_write_bytes_1ef3de': 4730, 'ec2_cpu_utilization_77c1ca': 4032, 'iio_us-east-1_i-a2eb1cd9_NetworkIn': 1243, 'grok_asg_anomaly': 4621, 'exchange-3_cpm_results': 1538, 'TravelTime_387': 2500, 'TravelTime_451': 2162, 'Twitter_volume_UPS': 15866, 'Twitter_volume_IBM': 15893, 'Twitter_volume_AAPL': 15902, 'Twitter_volume_AMZN': 15831, 'Twitter_volume_PFE': 15858, 'Twitter_volume_GOOG': 15842}\n"
     ]
    }
   ],
   "source": [
    "print(length_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ps-_4ROUPiuL"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def ReadDataset_rep(_file_name,rep,root,dir, _normalize=True):\n",
    "\n",
    "\n",
    "    df = pd.read_csv(_file_name,header=None)\n",
    "    abnormal_data = df.values\n",
    "\n",
    "    # Normal = 1, Abnormal = -1\n",
    "    \n",
    "    return abnormal_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2F1rCVI8E-5v"
   },
   "outputs": [],
   "source": [
    "def apprecons(y_pred_,length,width):\n",
    "  y_pred=np.zeros(len(abnormal_data)*len(abnormal_data[0]))\n",
    "  count=np.zeros(len(abnormal_data)*len(abnormal_data[0]))\n",
    "  j=0\n",
    "\n",
    "  for i in range(len(y_pred_)):\n",
    "    temp=np.copy(count)\n",
    "    count[j:(width+j)]+=y_pred_[i]\n",
    "    j+=1\n",
    "\n",
    "    \n",
    "  y_pred=np.sign(count)\n",
    "  # y_pred=np.where(y_pred==0, -1, y_pred)\n",
    "  y_pred[y_pred ==0] = -1\n",
    "\n",
    "  # y_pred=np.where(y_pred==0, 1, y_pred)\n",
    "  return y_pred[0:length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9TlctEbrwR8"
   },
   "outputs": [],
   "source": [
    "def apprecons1(score_,length,width):\n",
    "  score=np.zeros(len(score_)*len(score_[0]))\n",
    "  count=np.ones(len(score_)*len(score_[0]))\n",
    "  j=0\n",
    "  for i in range(len(score_)):\n",
    "    temp=np.copy(score)\n",
    "    score[j:(len(score_[i])+j)]+=score_[i]\n",
    "    \n",
    "#     index=np.argwhere(score!=temp)\n",
    "    j+=1\n",
    "#     count[index]+=1\n",
    "#   # print(len(np.argwhere(count[0:length]==0)))\n",
    "    \n",
    "  score/=count\n",
    "#   weights = [0.1,0.15,0.2,0.1,0.2,0.15,0.1]\n",
    "#   weights = [0.35,0.3,0.35] \n",
    "  weights = [0.2,0.25,0.1,0.25,0.2]\n",
    "\n",
    "#   score = np.convolve(score,np.array(weights)[::-1],'same')\n",
    "  return score[0:length]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNMuAjkjPVg_"
   },
   "source": [
    "# **NN Method warp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 3010,
     "status": "ok",
     "timestamp": 1584106711240,
     "user": {
      "displayName": "Abilasha S",
      "photoUrl": "",
      "userId": "17146042062219803973"
     },
     "user_tz": -330
    },
    "id": "A9zOtvnrPZuY",
    "outputId": "fbdc3386-e30e-4c41-8b94-e64423c885d4",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root= /home/abilasha/Downloads/NAB/data_warp/\n",
      "dir= realTweets\n",
      "['Twitter_volume_UPS.csv', 'Twitter_volume_IBM.csv', 'Twitter_volume_AAPL.csv', 'Twitter_volume_AMZN.csv', 'Twitter_volume_PFE.csv', 'Twitter_volume_GOOG.csv']\n",
      "filename /home/abilasha/Downloads/NAB/data_warp/realTweets/copy/Twitter_volume_UPS_warp_rep.csv\n",
      "/home/abilasha/Downloads/NAB/data_warp/realTweets/Twitter_volume_UPS.csv\n"
     ]
    }
   ],
   "source": [
    "def RunModel(_file_name,root,dir,lenth):\n",
    "    rep=1\n",
    "    abnormal_data = ReadDataset_rep(_file_name,rep,root,dir) \n",
    "    y_pred_arr1= np.empty((0,lenth), int)\n",
    "    y_predtemp_1=np.zeros((len(abnormal_data),bucket_size))\n",
    "    y_pred_1=-1*np.ones(lenth,int)\n",
    "    score_1=np.zeros(lenth)\n",
    "    y_pred_arr2= np.empty((0,lenth), int)\n",
    "    y_predtemp_2=np.zeros((len(abnormal_data),bucket_size))\n",
    "    score_2=np.zeros(lenth)\n",
    "    y_pred_2=-1*np.ones(lenth,int)\n",
    "    y_pred_arr3= np.empty((0,lenth), int)\n",
    "    y_predtemp_3=np.zeros((len(abnormal_data),bucket_size))\n",
    "    y_pred_3=-1*np.ones(lenth,int)\n",
    "    score_3=np.zeros(lenth)\n",
    "    \n",
    "    d_nn1=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn2=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn3=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    # d_nn3_arg=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    # d_nn3_max=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    # d_nn3_arg_max=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    w1=[]\n",
    "    w2=[]\n",
    "    w3=[]\n",
    "    k=0\n",
    "\n",
    "    mid=math.ceil(bucket_size/2)\n",
    "    \n",
    "#     clf = lof.LocalOutlierFactor(n_neighbors=1, n_jobs=2,contamination=0.1)\n",
    "#     y_temp = clf._fit_predict(abnormal_data)\n",
    "#     score=clf.negative_outlier_factor_\n",
    "#     dist=-1*score\n",
    "#     d_nn1=dist \n",
    "#     clf = lof.LocalOutlierFactor(n_neighbors=3, n_jobs=2,contamination=0.1)\n",
    "#     y_temp = clf._fit_predict(abnormal_data)\n",
    "#     score=clf.negative_outlier_factor_\n",
    "#     dist=-1*score \n",
    "#     d_nn2=dist\n",
    "#     clf = lof.LocalOutlierFactor(n_neighbors=5, n_jobs=2,contamination=0.1)\n",
    "#     y_temp = clf._fit_predict(abnormal_data)\n",
    "#     score=clf.negative_outlier_factor_\n",
    "#     dist=-1*score\n",
    "#     d_nn3=dist\n",
    "\n",
    "    for r,t in zip(abnormal_data,range(len(abnormal_data))):\n",
    "            dist=[]\n",
    "\n",
    "            dist_arg=[]\n",
    "            for o in abnormal_data:\n",
    "                    dist.append(np.linalg.norm(r-o))              \n",
    "                    \n",
    "            dist_arg=(np.argsort(dist))\n",
    "            dist=np.sort(dist)\n",
    "            \n",
    "            d_nn1[k]=dist[1]\n",
    "            d_nn2[k]=dist[3]\n",
    "            d_nn3[k]=dist[5]\n",
    "#......................score to mid....................................            \n",
    "            y_predtemp_1[k][mid-2:mid+2]=dist[1]\n",
    "            y_predtemp_2[k][mid-2:mid+2]=dist[3]\n",
    "            y_predtemp_3[k][mid-2:mid+2]=dist[5]\n",
    "#.....................................................................\n",
    "            # d_nn3_arg[k]=dist_arg[1]\n",
    "            # d_nn3_max[k]=dist[-1]\n",
    "            # d_nn3_arg_max[k]=dist_arg[-1]\n",
    "            # # d_nn3[k]=dist[9]\n",
    "\n",
    "            k+=1\n",
    "    \n",
    "    score_1=apprecons1(y_predtemp_1,lenth,bucket_size)\n",
    "    score_2=apprecons1(y_predtemp_2,lenth,bucket_size)\n",
    "    score_3=apprecons1(y_predtemp_3,lenth,bucket_size)\n",
    "    \n",
    "    return  score_1,score_2,score_3\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "              \n",
    "              for root, dirs, files in os.walk('/dataset/data_warp/'):\n",
    "                    dirs=['realKnownCause','realAWSCloudwatch','realAdExchange','realTraffic','realTweets']\n",
    "#                     dirs=['artificial']\n",
    "                    for dir in dirs:\n",
    "                        print(\"root=\",root)\n",
    "                        print(\"dir=\",dir)\n",
    "                        warp=\"copy\"\n",
    "                        for root1,dir1,files in os.walk(os.path.join(root,dir)):\n",
    "                            \n",
    "#                              files=['artificial_point_warp_rep.csv']\n",
    "                            print(files)\n",
    "                            for file in files:\n",
    "#                                  if file.endswith(\"_warp_rep.csv\"): \n",
    "                                dir_=os.path.join(root,dir,warp)\n",
    "#                          \n",
    "                                file_name = os.path.join(dir_,file.split('.csv')[0]+'_warp_rep.csv')\n",
    "                                print(\"filename\",file_name)\n",
    "                                print(os.path.join(root,dir,file))\n",
    "                                score_1,score_2,score_3=RunModel(file_name,root,dir,length_data[file.split('.csv')[0]])\n",
    "                                abnormal_data,lenth,abnormal_label=ReadNABDataset(os.path.join(root,dir,file))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, score_1)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, score_1)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                \n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, score_2)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                               \n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, score_3)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "\n",
    "                            break\n",
    "                               \n",
    "                    break         \n",
    "                                \n",
    "                                \n",
    "                                 \n",
    "                                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nwGhfZMxs99m"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NAB_datatowarp_point.ipynb",
   "provenance": [
    {
     "file_id": "1wYazsAo5mHQxJbaTNSulPUfnY5kJu0xT",
     "timestamp": 1576734667299
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
