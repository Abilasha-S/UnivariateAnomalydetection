{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install window-slider\n",
    "!pip install dtaidistance\n",
    "from dtaidistance import dtw\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import scipy.io\n",
    "from numpy import linalg as LA\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import join, getsize\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, roc_auc_score, precision_recall_curve, average_precision_score, cohen_kappa_score,confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from window_slider import Slider\n",
    "import sys\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "np.set_printoptions(suppress=True)\n",
    "from sklearn.neighbors import lof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQFEt9bxlF8U"
   },
   "outputs": [],
   "source": [
    "bucket_size = 30\n",
    "# bucket_size = 25\n",
    "length_data=dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9hCv5DviWfwc"
   },
   "outputs": [],
   "source": [
    "def ReadDataset_withlab(_file_name, _normalize=True):\n",
    "\n",
    "    abnormal = pd.read_csv(_file_name)\n",
    "    abnormal=abnormal.values\n",
    "\n",
    "    abnormal_data = abnormal[:,0]\n",
    "    abnormal_label = abnormal[:,1]\n",
    "\n",
    "  \n",
    "    return abnormal_data,abnormal_label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07nMc1ylhKL-"
   },
   "outputs": [],
   "source": [
    "def ReadDataset_seq(_file_name, _normalize=True):\n",
    "\n",
    "\n",
    "    df = pd.read_csv(_file_name,header=None)\n",
    "    abnormal = df.values\n",
    "\n",
    "    abnormal_data = abnormal[:,:-1] #change as position of class labels\n",
    "#     abnormal_data = abnormal[:,1:]\n",
    "    \n",
    "\n",
    "    abnormal_label = abnormal[:,-1]\n",
    "#     abnormal_label = abnormal[:,0]\n",
    "\n",
    "    print(\"anomaly\",(len(np.where(abnormal_label==-1)[0])))\n",
    "\n",
    "\n",
    "    # Normal = 1, Abnormal = -1\n",
    "    \n",
    "    return abnormal_data,len(abnormal_data),abnormal_label\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReadDataset_roll(_file_name_annot,anom_size):\n",
    "    bucket_size=50\n",
    "    base=os.path.basename(_file_name_annot) \n",
    "    base_name=base.split('Annotations_')[1]\n",
    "    file_name=os.path.join(os.path.dirname(_file_name_annot),base_name)\n",
    "    label_new=[]\n",
    "    value=[]\n",
    "    with open(file_name) as infile:\n",
    "        value = [float(line.strip('\\n')) for line in infile if line]\n",
    "    abnormal_data=np.array(value)\n",
    "    print(\"org len\",len(abnormal_data))\n",
    "    \n",
    "    with open(_file_name_annot) as infile:\n",
    "            indice = [int(line.strip('\\n')) for line in infile if line]\n",
    "        \n",
    "    abnormal_label=np.ones(len(abnormal_data))\n",
    "\n",
    "    for i in indice:\n",
    "                abnormal_label[i:i+anom_size]=-1\n",
    "       \n",
    "#     bucket_size=anom_size+20  \n",
    "    abnorm_data=np.empty((0,bucket_size),float)\n",
    "    abnorm_label=np.empty((0,bucket_size),float)\n",
    "    overlap_count = 0\n",
    "    slider = Slider(bucket_size,overlap_count)\n",
    "    slider1 = Slider(bucket_size,overlap_count)\n",
    "    slider.fit(abnormal_data) \n",
    "    slider1.fit(abnormal_label)\n",
    "    plt.plot(abnormal_data)\n",
    "    plt.show()\n",
    "    plt.plot(abnormal_label)\n",
    "    plt.show()\n",
    "    while True:\n",
    "        window_data1 = slider.slide() \n",
    "        window_label1= slider1.slide()\n",
    "        if  (len(window_data1)<bucket_size) and len(window_data1)>bucket_size/2:\n",
    "          window_data=np.pad(window_data1,[0,(bucket_size-len(window_data1))])\n",
    "          window_label=np.pad(window_label1,[0,(bucket_size-len(window_label1))])\n",
    "          abnorm_data= np.append(abnorm_data,[window_data], axis=0)\n",
    "          abnorm_label= np.append(abnorm_label,[window_label], axis=0)\n",
    "          break\n",
    "        else:\n",
    "          if len(window_data1)==0 or len(window_data1)<bucket_size/2:\n",
    "                break\n",
    "          abnorm_data=np.append(abnorm_data,[window_data1], axis=0)\n",
    "          abnorm_label=np.append(abnorm_label,[window_label1], axis=0)\n",
    "            \n",
    "    label=np.sign(np.sum(abnorm_label,axis=1))\n",
    "    combined_data=np.column_stack((label,abnorm_data))\n",
    "\n",
    "    return combined_data, anom_count\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "              anom_track=dict()\n",
    "              for root, dirs, files in os.walk('/dataset/discorddataset/'):\n",
    "                    for dir in dirs:                        \n",
    "                        print(\"root=\",root)\n",
    "                        print(\"dir=\",dir)  \n",
    "                        anom_dict={'Marotta_Valve_Tek_14': 128,'Marotta_Valve_Tek_17': 128, 'chfdbchf15': 200, 'ann_gun_CentroidA_1': 150, 'Patient_respiration2': 150, 'Patient_respiration': 100, 'Marotta_Valve_Tek_16': 128, 'dutch_power_demand': 800}\n",
    "                   \n",
    "                        for file in glob.glob(str(root+dir+'/Annotations_*.txt')): \n",
    "                                        base=os.path.basename(file)\n",
    "                                        base=base.split('Annotations_')[1]\n",
    "                                        base=base.split('.txt')[0]\n",
    "                                        print(file)\n",
    "                                        abnormal_data, a_count = ReadDataset_roll(file,anom_dict[base])\n",
    "                                        anom_track[base]=a_count\n",
    "                                        print(np.shape(abnormal_data))\n",
    "                                        np.savetxt(os.path.join(root,dir,str(base+'.csv')), abnormal_data, delimiter=',',fmt='%10.5f')\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "              for root, dirs, files in os.walk('/dataset/data_warp'):\n",
    "#                     dirs=['artificial_seq']  ######change label position new and old\n",
    "                    dirs=['seq_data']\n",
    "#                     dirs=['Wafer','HandOutlines']\n",
    "#                     dirs=['DistalPhalanxOutlineCorrect','MiddlePhalanxOutlineCorrect','ProximalPhalanxOutlineCorrect','Earthquakes','PhalangesOutlinesCorrect','Strawberry','ToeSegmentation1','ToeSegmentation2','DodgerLoopGame','ECG5000','SyntheticControl','ECG200','Wafer','HandOutlines']\n",
    "#                     dirs=['TwoLeadECG','Computers','Worms','Yoga','Ford','Lightning2','InsectWingbeatSound']\n",
    "                    for dir in dirs:\n",
    "                        \n",
    "                        s_precision = []\n",
    "                        s_recall = []\n",
    "                        s_f1 = []\n",
    "                        s_roc_auc = []\n",
    "                        s_pr_auc = []\n",
    "                        s_cks = []\n",
    "                        \n",
    "                        print(\"root=\",root)\n",
    "                        print(\"dir=\",dir)\n",
    "                        \n",
    "                        for root1,dir1,files in os.walk(os.path.join(root,dir)):\n",
    "                             print(files)\n",
    "                             \n",
    "                             for file in files: \n",
    "                                if file.endswith('.csv'):\n",
    "                                    file_name = os.path.join(root,dir,file)\n",
    "                                    print(file_name)\n",
    "                                \n",
    "                                # abnormal_data,length ,abnormal_label = ReadDataset_withlab(file_name)\n",
    "                                    abnormal_data,length,abnormal_label = ReadDataset_seq(file_name)\n",
    "                                    length_data[file.split('.csv')[-2]]=length\n",
    "                                    print(np.shape(abnormal_data))\n",
    "                                # print(np.shape(abnormal_label))\n",
    "                                    file_name1=os.path.join(os.path.join(root,dir),\"out_warp\",str(os.path.splitext(file)[0])+\"_warp.csv\")\n",
    "                                    np.savetxt(file_name1, abnormal_data, delimiter=',',fmt='%10.5f')\n",
    "#                                     file_name2=os.path.join(os.path.join(root,dir),\"out_warp\",str(os.path.splitext(file)[0])+\"_warp_label.csv\")\n",
    "#                                 # # file_name2=os.path.join(os.path.join(root,dir),\"out_warp\",str(os.path.splitext(file)[0])+\"_warp_revlabel.csv\")\n",
    "#                                     np.savetxt(file_name2, abnormal_label, delimiter=',',fmt='%d')\n",
    "                             break    \n",
    "                    break\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ps-_4ROUPiuL"
   },
   "outputs": [],
   "source": [
    "def ReadDataset_rep(_file_name,rep,root,dir, _normalize=True):\n",
    "\n",
    "\n",
    "    df = pd.read_csv(_file_name,header=None)\n",
    "    abnormal_data = df.values\n",
    "\n",
    "    \n",
    "    return abnormal_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "renaming files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "              \n",
    "              for root, dirs, files in os.walk('/wartem/representations/'):\n",
    "\n",
    "                    for dir in dirs:\n",
    "                        print(\"root=\",root)\n",
    "                        print(\"dir=\",dir)\n",
    "                        warp=\"interpolation\"\n",
    "                        for root1,dir1,files in os.walk(os.path.join(root,dir,warp)):\n",
    "                            print(files)\n",
    "                            for file in files:\n",
    "                                file_name = str(dir+'_rep.csv')\n",
    "                                print(file_name)\n",
    "                                os.rename(root+dir+'/'+warp+'/'+file,root+dir+'/'+warp+'/'+file_name)\n",
    "                                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNMuAjkjPVg_"
   },
   "source": [
    "# **NN Method warp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def RunModel(_file_name,root,dir,lenth):\n",
    "    rep=1\n",
    "    abnormal_data = ReadDataset_rep(_file_name,rep,root,dir) \n",
    "    y_pred_arr1= np.empty((0,lenth), int)\n",
    "    y_pred_1=-1*np.ones(lenth,int)\n",
    "    score_1=np.zeros(lenth)\n",
    "    y_pred_arr2= np.empty((0,lenth), int)\n",
    "\n",
    "    score_2=np.zeros(lenth)\n",
    "    y_pred_2=-1*np.ones(lenth,int)\n",
    "    y_pred_arr3= np.empty((0,lenth), int)\n",
    "\n",
    "    y_pred_3=-1*np.ones(lenth,int)\n",
    "    score_3=np.zeros(lenth)\n",
    "\n",
    "   \n",
    "    \n",
    "    d_nn1=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn2=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    d_nn3=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    \n",
    "    # d_nn3_arg=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    # d_nn3_max=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    # d_nn3_arg_max=[np.array(0) for i in range(len(abnormal_data))]\n",
    "    w1=[]\n",
    "    w2=[]\n",
    "    w3=[]\n",
    " \n",
    "    k=0\n",
    "    start_time = time.time()\n",
    "\n",
    "    \n",
    "#     clf = lof.LocalOutlierFactor(n_neighbors=1, n_jobs=2,contamination=0.1)\n",
    "#     y_temp = clf._fit_predict(abnormal_data)\n",
    "#     score=clf.negative_outlier_factor_\n",
    "#     dist=-1*score\n",
    "#     d_nn1=dist \n",
    "#     clf = lof.LocalOutlierFactor(n_neighbors=3, n_jobs=2,contamination=0.1)\n",
    "#     y_temp = clf._fit_predict(abnormal_data)\n",
    "#     score=clf.negative_outlier_factor_\n",
    "#     dist=-1*score \n",
    "#     d_nn2=dist\n",
    "#     clf = lof.LocalOutlierFactor(n_neighbors=5, n_jobs=2,contamination=0.1)\n",
    "#     y_temp = clf._fit_predict(abnormal_data)\n",
    "#     score=clf.negative_outlier_factor_\n",
    "#     dist=-1*score\n",
    "#     d_nn3=dist\n",
    "\n",
    "\n",
    "    for r,t in zip(abnormal_data,range(len(abnormal_data))):                    ####knn\n",
    "            neigh = NearestNeighbors(n_neighbors=5)\n",
    "            neigh.fit(abnormal_data)\n",
    "\n",
    "            d_nn1[k]=(neigh.kneighbors([r])[0])[0][1] \n",
    "            d_nn2[k]=(neigh.kneighbors([r])[0])[0][2] \n",
    "            d_nn3[k]=(neigh.kneighbors([r])[0])[0][4] \n",
    "          \n",
    "\n",
    "            # d_nn3_max[k]=dist[-1]\n",
    "            # d_nn3_arg_max[k]=dist_arg[-1]\n",
    "            # # d_nn3[k]=dist[9]\n",
    "\n",
    "            k+=1                                             ##knn\n",
    "   \n",
    "    return  d_nn1,d_nn2,d_nn3\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "              \n",
    "              for root, dirs, files in os.walk('/dataset/data_warp/'):\n",
    "                     dirs=['seq_data']\n",
    "#                      dirs=['artificial_seq']\n",
    "#                      dirs=['DistalPhalanxOutlineCorrect','MiddlePhalanxOutlineCorrect','ProximalPhalanxOutlineCorrect','Earthquakes','PhalangesOutlinesCorrect','Strawberry','ToeSegmentation1','ToeSegmentation2','DodgerLoopGame','ECG5000','SyntheticControl','ECG200','HandOutlines','Wafer']\n",
    "#                     dirs=['TwoLeadECG','Computers','Worms','Yoga','Ford','Lightning2','InsectWingbeatSound']\n",
    "                     for dir in dirs:\n",
    "                        \n",
    "                        \n",
    "                        print(\"root=\",root)\n",
    "                        print(\"dir=\",dir)\n",
    "                        warp=\"copy\"\n",
    "                        dir_=os.path.join(root,dir,warp)\n",
    "                        \n",
    "                        for root1,dir1,files in os.walk(dir_):\n",
    "                            print(files)\n",
    "#                             files=['dutch_power_demand_warp_rep.csv', 'ann_gun_CentroidA_1_warp_rep.csv', 'Patient_respiration2_warp_rep.csv', 'chfdbchf15_warp_rep.csv', 'Marotta_Valve_Tek_16_warp_rep.csv', 'Patient_respiration_warp_rep.csv', 'Marotta_Valve_Tek_17_warp_rep.csv','Marotta_Valve_Tek_14_warp_rep.csv']\n",
    "                            for file in files:\n",
    "                              \n",
    "                              \n",
    "                              if file.endswith(\"_warp_rep.csv\"): \n",
    "                                file_name = os.path.join(dir_,file)\n",
    "                                print(\"filename\",file_name)\n",
    "                               \n",
    "                                dist_1,dist_2,dist_3 = RunModel(file_name,root,dir,length_data[file.split('_warp_rep.csv')[-2]])\n",
    "                                abnormal_data,length,abnormal_label = ReadDataset_seq(os.path.join(root,dir,str(file.split('_warp_rep.csv')[0]+'.csv')))\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_1)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_1)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                auc_1=metrics.auc(recall,precision)\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_2)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_2)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                auc_3=metrics.auc(recall,precision)\n",
    "                                precision, recall, thresholds = metrics.precision_recall_curve(abnormal_label, dist_3)\n",
    "                                fpr, tpr, thresholds = metrics.roc_curve(abnormal_label, dist_3)\n",
    "                                print(metrics.auc(recall,precision),metrics.auc(fpr, tpr))\n",
    "                                auc_5=metrics.auc(recall,precision)\n",
    "                            \n",
    "                                mean_auc=np.mean([auc_1,auc_3,auc_5])\n",
    "                                print(mean_auc)\n",
    "\n",
    "                                \n",
    "                            break\n",
    "                               \n",
    "                     break         \n",
    "                                \n",
    "                                "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NAB_datatowarp.ipynb",
   "provenance": [
    {
     "file_id": "1wYazsAo5mHQxJbaTNSulPUfnY5kJu0xT",
     "timestamp": 1576734667299
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
